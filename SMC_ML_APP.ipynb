{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN9pFS3Ao2vWRHP+vWW6gxV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MIARD/SMC/blob/main/SMC_ML_APP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HYRo15yGj33S"
      },
      "outputs": [],
      "source": [
        "# prompt: import python required library for a data analysis\n",
        "# import time\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "# import plotly.express as px\n",
        "# import plotly.graph_objects as go"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "# from xgboost import XGBClassifier\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "# from sklearn.feature_selection import RFE\n",
        "# from sklearn.ensemble import VotingClassifier\n",
        "# from sklearn.ensemble import StackingClassifier\n",
        "# from sklearn.pipeline import Pipeline\n",
        "# from sklearn.pipeline import make_pipeline\n",
        "# from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "3hvUOPHeaAvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tabulate import tabulate\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "2RWwsewVKBsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df = pd.read_excel('SSL_T.xlsx')\n",
        "# df = pd.read_excel('SSL_T_MR.xlsx')\n",
        "# df = pd.read_excel('SSL_M.xlsx')\n",
        "\n",
        "# df = pd.read_excel('Train.xlsx')\n",
        "# df = pd.read_excel('Test.xlsx')\n",
        "# df = pd.read_excel('Train_Test.xlsx')\n",
        "\n",
        "pd.set_option('display.max_rows', None)"
      ],
      "metadata": {
        "id": "UkItsqU2kQ-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Overall Ratio Function"
      ],
      "metadata": {
        "id": "dxCj5ma-lYlU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_feature_data(p_state, index,row):\n",
        "       # Retrieve stats for individual features\n",
        "      row_data = {}\n",
        "      time_15min = row['15Min']\n",
        "      trade_count_15m, pl_15m, win_rate_15m = precomputed_stats[('15Min', time_15min)]\n",
        "      row_data['15Min_Trade Count'] = trade_count_15m\n",
        "      row_data['15Min_Win Rate'] = win_rate_15m\n",
        "      row_data['15Min_Profit/Loss'] = pl_15m\n",
        "\n",
        "      hour = row['Hour']\n",
        "      trade_count_hour, pl_hour, win_rate_hour = precomputed_stats[('Hour', hour)]\n",
        "      row_data['Hour_Trade Count'] = trade_count_hour\n",
        "      row_data['Hour_Win Rate'] = win_rate_hour\n",
        "      row_data['Hour_Profit/Loss'] = pl_hour\n",
        "\n",
        "      weekday = row['Start_Weekday']\n",
        "      trade_count_weekday, pl_weekday, win_rate_weekday = precomputed_stats[('Weekday', weekday)]\n",
        "      row_data['Weekday_Trade Count'] = trade_count_weekday\n",
        "      row_data['Weekday_Win Rate'] = win_rate_weekday\n",
        "      row_data['Weekday_Profit/Loss'] = pl_weekday\n",
        "\n",
        "      month = row['Month']\n",
        "      trade_count_month, pl_month, win_rate_month = precomputed_stats[('Month', month)]\n",
        "      row_data['Month_Trade Count'] = trade_count_month\n",
        "      row_data['Month_Win Rate'] = win_rate_month\n",
        "      row_data['Month_Profit/Loss'] = pl_month\n",
        "\n",
        "      criteria = row['Criteria']\n",
        "      trade_count_criteria, pl_criteria, win_rate_criteria = precomputed_stats[('Criteria', criteria)]\n",
        "      row_data['Criteria_Trade Count'] = trade_count_criteria\n",
        "      row_data['Criteria_Win Rate'] = win_rate_criteria\n",
        "      row_data['Criteria_Profit/Loss'] = pl_criteria\n",
        "\n",
        "\n",
        "      # Retrieve stats for combined features\n",
        "      trade_count_weekday_15m, pl_weekday_15m, win_rate_weekday_15m = precomputed_stats[('Weekday_15Min', weekday, time_15min)]\n",
        "      row_data['Weekday_15Min_Trade Count'] = trade_count_weekday_15m\n",
        "      row_data['Weekday_15Min_Win Rate'] = win_rate_weekday_15m\n",
        "      row_data['Weekday_15Min_Profit/Loss'] = pl_weekday_15m\n",
        "\n",
        "      trade_count_weekday_hour, pl_weekday_hour, win_rate_weekday_hour = precomputed_stats[('Weekday_Hour', weekday, hour)]\n",
        "      row_data['Weekday_Hour_Trade Count'] = trade_count_weekday_hour\n",
        "      row_data['Weekday_Hour_Win Rate'] = win_rate_weekday_hour\n",
        "      row_data['Weekday_Hour_Profit/Loss'] = pl_weekday_hour\n",
        "\n",
        "      trade_count_weekday_month, pl_weekday_month, win_rate_weekday_month = precomputed_stats[('Weekday_Month', weekday, month)]\n",
        "      row_data['Weekday_Month_Trade Count'] = trade_count_weekday_month\n",
        "      row_data['Weekday_Month_Win Rate'] = win_rate_weekday_month\n",
        "      row_data['Weekday_Month_Profit/Loss'] = pl_weekday_month\n",
        "\n",
        "      trade_count_criteria_15m, pl_criteria_15m, win_rate_criteria_15m = precomputed_stats[('Criteria_15Min', criteria, time_15min)]\n",
        "      row_data['Criteria_15Min_Trade Count'] = trade_count_criteria_15m\n",
        "      row_data['Criteria_15Min_Win Rate'] = win_rate_criteria_15m\n",
        "      row_data['Criteria_15Min_Profit/Loss'] = pl_criteria_15m\n",
        "\n",
        "      trade_count_criteria_hour, pl_criteria_hour, win_rate_criteria_hour = precomputed_stats[('Criteria_Hour', criteria, hour)]\n",
        "      row_data['Criteria_Hour_Trade Count'] = trade_count_criteria_hour\n",
        "      row_data['Criteria_Hour_Win Rate'] = win_rate_criteria_hour\n",
        "      row_data['Criteria_Hour_Profit/Loss'] = pl_criteria_hour\n",
        "\n",
        "      trade_count_criteria_weekday, pl_criteria_weekday, win_rate_criteria_weekday = precomputed_stats[('Criteria_Weekday', criteria, weekday)]\n",
        "      row_data['Criteria_Weekday_Trade Count'] = trade_count_criteria_weekday\n",
        "      row_data['Criteria_Weekday_Win Rate'] = win_rate_criteria_weekday\n",
        "      row_data['Criteria_Weekday_Profit/Loss'] = pl_criteria_weekday\n",
        "\n",
        "      trade_count_criteria_month, pl_criteria_month, win_rate_criteria_month = precomputed_stats[('Criteria_Month', criteria, month)]\n",
        "      row_data['Criteria_Month_Trade Count'] = trade_count_criteria_month\n",
        "      row_data['Criteria_Month_Win Rate'] = win_rate_criteria_month\n",
        "      row_data['Criteria_Month_Profit/Loss'] = pl_criteria_month\n",
        "\n",
        "      trade_count_hour_month, pl_hour_month, win_rate_hour_month = precomputed_stats[('Hour_Month', hour, month)]\n",
        "      row_data['Hour_Month_Trade Count'] = trade_count_hour_month\n",
        "      row_data['Hour_Month_Win Rate'] = win_rate_hour_month\n",
        "      row_data['Hour_Month_Profit/Loss'] = pl_hour_month\n",
        "\n",
        "      trade_count_15min_month, pl_15min_month, win_rate_15min_month = precomputed_stats[('15Min_Month', time_15min, month)]\n",
        "      row_data['15Min_Month_Trade Count'] = trade_count_15min_month\n",
        "      row_data['15Min_Month_Win Rate'] = win_rate_15min_month\n",
        "      row_data['15Min_Month_Profit/Loss'] = pl_15min_month\n",
        "\n",
        "      return row_data\n"
      ],
      "metadata": {
        "id": "L5o9rUQov7Lb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_model_result(model_name, accuracy, class_report):\n",
        "    print(f\"\\nModel: {model_name}\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(class_report)"
      ],
      "metadata": {
        "id": "7rMTsrgZc7Eb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Get Train Data From Input"
      ],
      "metadata": {
        "id": "d6HOBC9hrIjA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: get_train_data_for_input(df, time_input, date_input, criteria_input): instead of this get_train_data_for_input_optimized(precompute_stats, time_input, date_input, criteria_input) so that it can take data from precompute state instead of df\n",
        "\n",
        "def get_train_data_for_input(precomputed_stats, time_input, date_input, criteria_input, training_feature_cols=[]):\n",
        "\n",
        "    # Convert inputs\n",
        "    try:\n",
        "        input_time = datetime.strptime(time_input, '%H:%M').time()\n",
        "    except ValueError:\n",
        "        print(\"Invalid time format. Use HH:MM.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    try:\n",
        "        input_date = datetime.strptime(date_input, '%m/%d/%Y')\n",
        "    except ValueError:\n",
        "        print(\"Invalid date format. Use M/D/YYYY.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    row_data = {}\n",
        "    input_datetime = datetime.combine(input_date, input_time)\n",
        "\n",
        "    time_15min = input_datetime.strftime('%H:%M')\n",
        "    hour = input_datetime.hour\n",
        "    weekday = input_datetime.strftime('%A')\n",
        "    month = input_datetime.strftime('%B')\n",
        "    criteria = criteria_input\n",
        "\n",
        "    # Retrieve stats from precomputed_stats dictionary\n",
        "    try:\n",
        "        row = {'15Min': time_15min, 'Hour': hour, 'Start_Weekday': weekday, 'Month': month, 'Criteria': criteria}\n",
        "        row_data = get_feature_data(precomputed_stats, 0, row)\n",
        "\n",
        "    except KeyError as e:\n",
        "        print(f\"Error: Missing precomputed statistic for key {e}. Ensure create_precomputed_stats covers all combinations you need.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "\n",
        "    # Create DataFrame from the calculated row_data\n",
        "    input_features_df = pd.DataFrame([row_data])\n",
        "    # Reindex the input_features_df to match the training columns, filling missing with 0\n",
        "    input_features_df = input_features_df.reindex(columns=training_feature_cols, fill_value=0)\n",
        "\n",
        "    # Convert all columns to the same data type as the training features (assuming int from previous steps)\n",
        "    for col in input_features_df.columns:\n",
        "        try:\n",
        "             input_features_df[col] = pd.to_numeric(input_features_df[col], errors='coerce')\n",
        "             input_features_df[col] = input_features_df[col].fillna(0).astype(int)\n",
        "        except ValueError:\n",
        "            print(f\"Warning: Could not convert input column '{col}' to integer.\")\n",
        "\n",
        "\n",
        "    return input_features_df\n",
        "\n",
        "# Example usage (assuming X_train is defined from the previous code):\n",
        "# Get the column names from your training features DataFrame\n",
        "\n",
        "# Call the optimized function\n",
        "# input_data = get_train_data_for_input_optimized(precomputed_stats, '09:30', '01/8/2023', 'ELC', training_feature_cols)\n",
        "# print(input_data)\n"
      ],
      "metadata": {
        "id": "9u1ALKrL9kRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Show Prediciton"
      ],
      "metadata": {
        "id": "E5V2f13WrWM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Prediction using the trained model ---\n",
        "def predict_trade_result(model, input_features_df):\n",
        "    \"\"\"\n",
        "    Predicts the result of a trade based on the input features using the trained model.\n",
        "\n",
        "    Args:\n",
        "        model: The trained machine learning model.\n",
        "        input_features_df (pd.DataFrame): A DataFrame with one row containing\n",
        "                                          the features for prediction, matching\n",
        "                                          the format of the training features.\n",
        "\n",
        "    Returns:\n",
        "        int: The predicted class (e.g., 1 for Win, 0 for Loss).\n",
        "        float: The predicted probability of the positive class (Win).\n",
        "    \"\"\"\n",
        "    if input_features_df.empty:\n",
        "        print(\"Cannot predict: Invalid input features.\")\n",
        "        return None, None\n",
        "\n",
        "    # Make prediction\n",
        "    predicted_class = model.predict(input_features_df)[0]\n",
        "    if predicted_class == 0:\n",
        "        predicted_proba = model.predict_proba(input_features_df)\n",
        "    else:\n",
        "        predicted_proba = model.predict_proba(input_features_df)\n",
        "\n",
        "    return predicted_class, predicted_proba"
      ],
      "metadata": {
        "id": "bXjEgBs5UmuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # pip install tabulate if needed\n",
        "\n",
        "def show_predictions(time_input, date_input, criteria_input, CW, CL, training_feature_cols, model_names, model_keys,b_models=None, m_models=None, b_acc=None, m_acc=None, Max_R=False, w_threshold=2):\n",
        "    mr = \"_max_r\" if Max_R else \"\"\n",
        "    # Initialize models and their keys\n",
        "    binary_models = b_models if b_models is not None else [globals()[f\"{k}_model{mr}\"] for k in model_keys]\n",
        "    binary_accuracies = b_acc if b_acc is not None else [globals()[f\"{k}_accuracy{mr}\"] for k in model_keys]\n",
        "\n",
        "    multiclass_models = m_models if m_models is not None else [globals()[f\"{k}_model_m{mr}\"] for k in model_keys]\n",
        "    multiclass_accuracies = m_acc if m_acc is not None else [globals()[f\"{k}_accuracy_m{mr}\"] for k in model_keys]\n",
        "\n",
        "    input_features = get_train_data_for_input(precomputed_stats, time_input, date_input, criteria_input, training_feature_cols)\n",
        "    input_features['CL'] = [CL]\n",
        "    input_features['CW'] = [CW]\n",
        "    predictions = []\n",
        "    predictions_multi = []\n",
        "    if not input_features.empty:\n",
        "        table_data = []\n",
        "\n",
        "        for i, name in enumerate(model_names):\n",
        "            # Predict binary\n",
        "            pred_bin, prob_bin = predict_trade_result(binary_models[i], input_features)\n",
        "            label_bin = 'Win' if pred_bin == 1 else 'Loss'\n",
        "            acc_bin = f\"{binary_accuracies[i]:.2f}\"\n",
        "            bin_str = f\"{label_bin} ({np.max(prob_bin):.2f}), Acc: {acc_bin}\"\n",
        "\n",
        "            # Predict multiclass\n",
        "            pred_multi, prob_multi = predict_trade_result(multiclass_models[i], input_features)\n",
        "            acc_multi = f\"{multiclass_accuracies[i]:.2f}\"\n",
        "            multi_str = f\"Class {label_encoder.inverse_transform([pred_multi])[0]} ({np.max(prob_multi):.2f}), Acc: {acc_multi}\"\n",
        "            table_data.append([name, bin_str, multi_str])\n",
        "            predictions.append(pred_bin)\n",
        "            predictions_multi.append(pred_multi)\n",
        "        predicted_win = predictions.count(1)\n",
        "        predicted_loss = predictions.count(0)\n",
        "        predicted_win_multi = len(predictions)-predictions_multi.count(0)\n",
        "        predicted_loss_multi = predictions_multi.count(0)\n",
        "        trade_decision = \"Win\" if predicted_win > w_threshold else \"Loss\"\n",
        "        trade_decision_multi = \"Win\" if predicted_win_multi >= w_threshold else \"Loss\"\n",
        "        max_r = \"\" if trade_decision_multi == \"Loss\" else \"- Max R: \"+str(np.max(predictions_multi))\n",
        "        avg_r = \"\" if trade_decision_multi == \"Loss\" else \"- Average R:\"+str(np.mean([x for x in predictions_multi if x>0]))\n",
        "        # Display table\n",
        "        print(f\"\\n--- ğŸ“Š Combined Model Predictions for {date_input} {time_input} ({criteria_input}) -{mr} ---\\n\")\n",
        "        print(f\"Overall decision: ---- Binary: ** {trade_decision} **  Multi: ** {trade_decision_multi}** ---- {max_r} {avg_r}\")\n",
        "        print(f\"Binary Models: Win Predicted **{predicted_win} time **    Multi Models: Win Predicted **{predicted_win_multi}**\")\n",
        "        print(tabulate(table_data, headers=[\"Model\", \"ğŸ“˜ Binary (Win/Loss)\", \"ğŸ“— Multi-Class (R Bucket)\"], tablefmt=\"fancy_grid\"))\n",
        "\n",
        "    else:\n",
        "        print(\"âŒ Prediction could not be made due to invalid inputs or data.\")\n"
      ],
      "metadata": {
        "id": "FMTN_xdyVTPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Read Require Data"
      ],
      "metadata": {
        "id": "REMqQ7JqUyi-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Read Models"
      ],
      "metadata": {
        "id": "ETy_lAgrU44I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Read From Drive"
      ],
      "metadata": {
        "id": "1TlL6vo3JhEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "MYbNFeahJcm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: remove SMC directory from my files\n",
        "%cd /content/\n",
        "!rm -rf SMC/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fF4KtKXOH8JJ",
        "outputId": "a7128652-7de4-4081-eb3c-da4b82cf132e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy SMC folder from Drive to current working directory (optional)\n",
        "!cp -r /content/drive/MyDrive/SMC /content/\n",
        "\n",
        "# Move into the project directory\n",
        "%cd /content/SMC\n",
        "\n",
        "# List contents to confirm\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFtIkA05JgHj",
        "outputId": "92f70fd5-da52-48c1-9fcd-70917e9d52d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat '/content/drive/MyDrive/SMC': No such file or directory\n",
            "[Errno 2] No such file or directory: '/content/SMC'\n",
            "/content\n",
            "sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Read From Git"
      ],
      "metadata": {
        "id": "8Oo8913xJjcl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the GitHub repository\n",
        "!git clone https://github.com/MIARD/SMC.git\n",
        "\n",
        "# Move into the cloned directory\n",
        "%cd SMC\n",
        "\n",
        "# List files to confirm\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByL4dhZ45eKu",
        "outputId": "bd90b34b-cea6-49f8-fa62-ae2ecd299c29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SMC'...\n",
            "remote: Enumerating objects: 31, done.\u001b[K\n",
            "remote: Counting objects: 100% (31/31), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 31 (delta 16), reused 31 (delta 16), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (31/31), 6.73 MiB | 10.31 MiB/s, done.\n",
            "Resolving deltas: 100% (16/16), done.\n",
            "/content/SMC\n",
            "max_r_model_accuracies.xlsx    saved_models\n",
            "precomputed_stats.pkl\t       SSL_T_MR.xlsx\n",
            "README.md\t\t       SSL_T.xlsx\n",
            "regular_model_accuracies.xlsx  training_feature_cols.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Read info"
      ],
      "metadata": {
        "id": "r7DkoswOLCTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: read both accuracies from save xlxs and show their tables\n",
        "\n",
        "# Read the saved accuracy dataframes\n",
        "try:\n",
        "  precomputed_stats = joblib.load('precomputed_stats.pkl')\n",
        "  training_feature_cols = joblib.load('training_feature_cols.pkl')\n",
        "  regular_accuracies = pd.read_excel(\"regular_model_accuracies.xlsx\")\n",
        "  max_r_accuracies = pd.read_excel(\"max_r_model_accuracies.xlsx\")\n",
        "\n",
        "  print(\"\\n--- Regular Model Accuracies ---\")\n",
        "  print(tabulate(regular_accuracies, headers='keys', tablefmt='psql'))\n",
        "\n",
        "  print(\"\\n--- Max R Model Accuracies ---\")\n",
        "  print(tabulate(max_r_accuracies, headers='keys', tablefmt='psql'))\n",
        "\n",
        "except FileNotFoundError:\n",
        "  print(\"Accuracy files not found. Please run the model training section first.\")\n",
        "except Exception as e:\n",
        "  print(f\"An error occurred while reading the accuracy files: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4dI44CFLBwW",
        "outputId": "41073efc-a178-4355-bfda-ea2f3733bcf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Regular Model Accuracies ---\n",
            "+----+---------------------+-------------------+-----------------------------------+\n",
            "|    | Model               |   Binary Accuracy |   Multi-Class (R Bucket) Accuracy |\n",
            "|----+---------------------+-------------------+-----------------------------------|\n",
            "|  0 | RandomForest        |          0.878689 |                          0.790164 |\n",
            "|  1 | XGBoost             |          0.852459 |                          0.796721 |\n",
            "|  2 | Logistic Regression |          0.898361 |                          0.829508 |\n",
            "|  3 | Voting Classifier   |          0.885246 |                          0.819672 |\n",
            "|  4 | Stacking Classifier |          0.898361 |                          0.829508 |\n",
            "+----+---------------------+-------------------+-----------------------------------+\n",
            "\n",
            "--- Max R Model Accuracies ---\n",
            "+----+---------------------------+-------------------+-----------------------------------+\n",
            "|    | Model                     |   Binary Accuracy |   Multi-Class (R Bucket) Accuracy |\n",
            "|----+---------------------------+-------------------+-----------------------------------|\n",
            "|  0 | RandomForest Max R        |          0.878689 |                          0.721311 |\n",
            "|  1 | XGBoost Max R             |          0.852459 |                          0.718033 |\n",
            "|  2 | Logistic Regression Max R |          0.898361 |                          0.731148 |\n",
            "|  3 | Voting Classifier Max R   |          0.885246 |                          0.727869 |\n",
            "|  4 | Stacking Classifier Max R |          0.898361 |                          0.731148 |\n",
            "+----+---------------------------+-------------------+-----------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Loading Models"
      ],
      "metadata": {
        "id": "Vx9wuV6GJoS4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: read all 20 models separately  regular binary, regular multi, max r binary, max r multi, . so that i can pass binary model and multi models list to function easily\n",
        "\n",
        "# Define a directory to save the models\n",
        "model_dir = 'saved_models'\n",
        "\n",
        "# Define lists to store the loaded models\n",
        "regular_binary_models = []\n",
        "regular_multi_models = []\n",
        "max_r_binary_models = []\n",
        "max_r_multi_models = []\n",
        "\n",
        "# List of model base names (keys used in filenames)\n",
        "model_keys = ['RF', 'XVG', 'LR', 'VC', 'S']\n",
        "\n",
        "# Load regular binary models\n",
        "print(\"Loading regular binary models...\")\n",
        "for key in model_keys:\n",
        "    filename = os.path.join(model_dir, f'{key}_model.pkl')\n",
        "    try:\n",
        "        model = joblib.load(filename)\n",
        "        regular_binary_models.append(model)\n",
        "        print(f\"Loaded {key}_model\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: {key}_model.pkl not found. Please run the saving section first.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {key}_model: {e}\")\n",
        "\n",
        "# Load regular multi-class models\n",
        "print(\"\\nLoading regular multi-class models...\")\n",
        "for key in model_keys:\n",
        "    filename = os.path.join(model_dir, f'{key}_model_m.pkl')\n",
        "    try:\n",
        "        model = joblib.load(filename)\n",
        "        regular_multi_models.append(model)\n",
        "        print(f\"Loaded {key}_model_m\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: {key}_model_m.pkl not found. Please run the saving section first.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {key}_model_m: {e}\")\n",
        "\n",
        "# Load max R binary models\n",
        "print(\"\\nLoading Max R binary models...\")\n",
        "for key in model_keys:\n",
        "    filename = os.path.join(model_dir, f'{key}_model_max_r.pkl')\n",
        "    try:\n",
        "        model = joblib.load(filename)\n",
        "        max_r_binary_models.append(model)\n",
        "        print(f\"Loaded {key}_model_max_r\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: {key}_model_max_r.pkl not found. Please run the saving section first.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {key}_model_max_r: {e}\")\n",
        "\n",
        "# Load max R multi-class models\n",
        "print(\"\\nLoading Max R multi-class models...\")\n",
        "for key in model_keys:\n",
        "    filename = os.path.join(model_dir, f'{key}_model_m_max_r.pkl')\n",
        "    try:\n",
        "        model = joblib.load(filename)\n",
        "        max_r_multi_models.append(model)\n",
        "        print(f\"Loaded {key}_model_m_max_r\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: {key}_model_m_max_r.pkl not found. Please run the saving section first.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {key}_model_m_max_r: {e}\")\n",
        "\n",
        "# Load Label Encoders\n",
        "print(\"\\nLoading Label Encoders...\")\n",
        "try:\n",
        "    label_encoder = joblib.load(os.path.join(model_dir, 'label_encoder.pkl'))\n",
        "    print(\"Loaded label_encoder\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: label_encoder.pkl not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading label_encoder: {e}\")\n",
        "\n",
        "try:\n",
        "    label_encoder_max_r = joblib.load(os.path.join(model_dir, 'label_encoder_max_r.pkl'))\n",
        "    print(\"Loaded label_encoder_max_r\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: label_encoder_max_r.pkl not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading label_encoder_max_r: {e}\")\n",
        "\n",
        "\n",
        "# Now you have the models loaded into these lists:\n",
        "# regular_binary_models\n",
        "# regular_multi_models\n",
        "# max_r_binary_models\n",
        "# max_r_multi_models\n",
        "\n",
        "# You can pass these lists to other functions as needed.\n",
        "# For example, you could create a function that takes a list of models and a dataframe\n",
        "# and performs predictions or evaluation.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vtlJirpHqOk",
        "outputId": "c94fdbb4-ce2e-4ec1-8b93-1fcbf4898a2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading regular binary models...\n",
            "Loaded RF_model\n",
            "Loaded XVG_model\n",
            "Loaded LR_model\n",
            "Loaded VC_model\n",
            "Loaded S_model\n",
            "\n",
            "Loading regular multi-class models...\n",
            "Loaded RF_model_m\n",
            "Loaded XVG_model_m\n",
            "Loaded LR_model_m\n",
            "Loaded VC_model_m\n",
            "Loaded S_model_m\n",
            "\n",
            "Loading Max R binary models...\n",
            "Loaded RF_model_max_r\n",
            "Loaded XVG_model_max_r\n",
            "Loaded LR_model_max_r\n",
            "Loaded VC_model_max_r\n",
            "Loaded S_model_max_r\n",
            "\n",
            "Loading Max R multi-class models...\n",
            "Loaded RF_model_m_max_r\n",
            "Loaded XVG_model_m_max_r\n",
            "Loaded LR_model_m_max_r\n",
            "Loaded VC_model_m_max_r\n",
            "Loaded S_model_m_max_r\n",
            "\n",
            "Loading Label Encoders...\n",
            "Loaded label_encoder\n",
            "Loaded label_encoder_max_r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Prediction App"
      ],
      "metadata": {
        "id": "TcKc6Ob3JsIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "time_input = \"18:30\"  #@param {type:\"string\"}\n",
        "date_input = \"6/29/2025\"  #@param {type:\"string\"}\n",
        "criteria_input = \"LG C\"  #@param {type:\"string\"}\n",
        "CW = 0 #@param {type:\"integer\"}\n",
        "CL = 3 #@param {type:\"integer\"}\n",
        "\n",
        "model_names = ['Random Forest', 'XGBoost', 'Logistic Regression', 'Voting Classifier', 'Stacking Classifier']\n",
        "model_names_max_r = ['Random Forest', 'XGBoost', 'Logistic Regression', 'Voting Classifier', 'Stacking Classifier']\n",
        "model_keys = ['RF', 'XVG', 'LR', 'VC', 'S']\n",
        "\n",
        "show_predictions(time_input, date_input, criteria_input, CW, CL,training_feature_cols, model_names, model_keys, b_models=regular_binary_models, m_models=regular_multi_models, b_acc=regular_accuracies['Binary Accuracy'], m_acc=regular_accuracies['Multi-Class (R Bucket) Accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "show_predictions(time_input, date_input, criteria_input, CW, CL,training_feature_cols, model_names_max_r, model_keys, b_models=max_r_binary_models, m_models=max_r_multi_models, b_acc=max_r_accuracies['Binary Accuracy'], m_acc=max_r_accuracies['Multi-Class (R Bucket) Accuracy'], Max_R=True)\n",
        "# List of predictions from the trained models\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2zJXVA3f_TA",
        "outputId": "fc9627ce-ff13-4d8b-9b4a-80d9638381d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- ğŸ“Š Combined Model Predictions for 6/29/2025 18:30 (LG C) - ---\n",
            "\n",
            "Overall decision: ---- Binary: ** Loss **  Multi: ** Win** ---- - Max R: 2 - Average R:2.0\n",
            "Binary Models: Win Predicted **0 time **    Multi Models: Win Predicted **2**\n",
            "â•’â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â••\n",
            "â”‚ Model               â”‚ ğŸ“˜ Binary (Win/Loss)   â”‚ ğŸ“— Multi-Class (R Bucket)   â”‚\n",
            "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
            "â”‚ Random Forest       â”‚ Loss (0.91), Acc: 0.88 â”‚ Class 1 (0.92), Acc: 0.79   â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ XGBoost             â”‚ Loss (1.00), Acc: 0.85 â”‚ Class 1 (1.00), Acc: 0.80   â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ Logistic Regression â”‚ Loss (0.88), Acc: 0.90 â”‚ Class 3 (0.43), Acc: 0.83   â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ Voting Classifier   â”‚ Loss (0.93), Acc: 0.89 â”‚ Class 1 (0.77), Acc: 0.82   â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ Stacking Classifier â”‚ Loss (0.97), Acc: 0.90 â”‚ Class 3 (0.72), Acc: 0.83   â”‚\n",
            "â•˜â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•›\n",
            "\n",
            "--- ğŸ“Š Combined Model Predictions for 6/29/2025 18:30 (LG C) -_max_r ---\n",
            "\n",
            "Overall decision: ---- Binary: ** Loss **  Multi: ** Loss** ----  \n",
            "Binary Models: Win Predicted **0 time **    Multi Models: Win Predicted **1**\n",
            "â•’â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â••\n",
            "â”‚ Model               â”‚ ğŸ“˜ Binary (Win/Loss)   â”‚ ğŸ“— Multi-Class (R Bucket)   â”‚\n",
            "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
            "â”‚ Random Forest       â”‚ Loss (0.91), Acc: 0.88 â”‚ Class 1 (0.94), Acc: 0.72   â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ XGBoost             â”‚ Loss (1.00), Acc: 0.85 â”‚ Class 1 (1.00), Acc: 0.72   â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ Logistic Regression â”‚ Loss (0.88), Acc: 0.90 â”‚ Class 1 (0.51), Acc: 0.73   â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ Voting Classifier   â”‚ Loss (0.93), Acc: 0.89 â”‚ Class 1 (0.82), Acc: 0.73   â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚ Stacking Classifier â”‚ Loss (0.97), Acc: 0.90 â”‚ Class 5 (0.82), Acc: 0.73   â”‚\n",
            "â•˜â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•›\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost\n",
        "print(xgboost.__version__)"
      ],
      "metadata": {
        "id": "2ifFdTqpKwG0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bd551f0-56d2-4a06-e35f-daf91284fa13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H-D-jAUAMH-v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}