# -*- coding: utf-8 -*-
"""SMC_ML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SGUJPTzzPV-aXF_FpaL_pe239F8-RJ8i
"""

# prompt: import python required library for a data analysis
import time
import joblib
from datetime import datetime
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from tabulate import tabulate

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import RFE
from sklearn.ensemble import VotingClassifier
from sklearn.ensemble import StackingClassifier
from sklearn.pipeline import Pipeline
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import LabelEncoder

df = pd.read_excel('SSL_T.xlsx')
# df = pd.read_excel('SSL_T_MR.xlsx')
# df = pd.read_excel('SSL_M.xlsx')

# df = pd.read_excel('Train.xlsx')
# df = pd.read_excel('Test.xlsx')
# df = pd.read_excel('Train_Test.xlsx')

pd.set_option('display.max_rows', None)

"""#Data Preparation"""

def prepare_data(df):
    df['startDateTime']=pd.to_datetime(df['Start Date'].astype(str) + ' ' + df['Start Time'].astype(str))
    # prompt: make Exit date and Exit time column as datetime. Time is on UTC.
    df['exitDateTime'] = pd.to_datetime(df['Exit Date'].astype(str) + ' ' + df['Exit Time'].astype(str),)
    df['Trade Duration (hours)'] = (df['exitDateTime'] - df['startDateTime']).dt.total_seconds() / 3600
    df['Start_Weekday'] = df['startDateTime'].dt.day_name()
    df['Exit_Weekday'] = df['exitDateTime'].dt.day_name()
    df['Profit/Loss']= df['Profit'] - df['Loss'] - df['Fee']
    df['Hour'] = df['startDateTime'].dt.hour
    df['Time'] = df['startDateTime'].dt.time
    df['15Min'] = df['startDateTime'].dt.strftime('%H:%M')
    df['Month'] = df['startDateTime'].dt.month_name()
    return df
df = prepare_data(df)
df.columns

def calculate_prior_streaks(df):
    cl_list = [3]  # First trade starts with CL = 3
    cw_list = [0]  # First trade starts with CW = 0

    for i in range(1, len(df)):
        prev_result = df.loc[i - 1, 'Result']
        prev_cl = cl_list[-1]
        prev_cw = cw_list[-1]

        if prev_result == 'W':
            cw = prev_cw + 1
            cl = 0
        elif prev_result == 'L':
            cl = prev_cl + 1
            cw = 0
        else:
            cl = prev_cl
            cw = prev_cw

        cl_list.append(cl)
        cw_list.append(cw)

    df['CL'] = cl_list
    df['CW'] = cw_list
    return df

# Example usage:
df = calculate_prior_streaks(df)

"""#Machine Learning

##Overall Ratio Function
"""

# --- Inputs ---
def Overall_R(df, time_input, date_input, criteria_input, with_overall=True):
# Convert inputs
    try:
        input_time = datetime.strptime(time_input, '%H:%M').time()
    except ValueError:
        print("Invalid time format. Use HH:MM.")
        input_time = None

    try:
        input_date = datetime.strptime(date_input, '%m/%d/%Y')
    except ValueError:
        print("Invalid date format. Use M/D/YYYY.")
        input_date = None

    results = []

    def analyze_and_collect(data_df, description):
        if not data_df.empty:
            total_trades = len(data_df)
            total_wins = (data_df['Profit/Loss'] > 0).sum()
            win_rate = total_wins / total_trades if total_trades > 0 else 0
            total_pl = data_df['Profit/Loss'].sum()

            results.append({
                "Description": description,
                "Trade Count": total_trades,
                "Win Rate": round(win_rate * 100, 2),
                "Profit/Loss": round(total_pl, 2)
            })

    # --- Main logic ---
    if input_time and input_date:
        input_hour = input_time.hour
        input_weekday = input_date.strftime('%A')
        input_month = input_date.strftime('%B')

        # Add your DataFrame here (df must be defined before running this)
        # df = pd.read_csv("your_file.csv")

        analyze_and_collect(df, "Overall Statistics")
        if '15Min' in df.columns:
            analyze_and_collect(df[df['15Min'] == time_input], f"Time: {time_input}")
        if 'Hour' in df.columns:
            analyze_and_collect(df[df['Hour'] == input_hour], f"Hour: {input_hour}")
        if 'Start_Weekday' in df.columns:
            analyze_and_collect(df[df['Start_Weekday'] == input_weekday], f"Weekday: {input_weekday}")
        if 'Month' in df.columns:
            analyze_and_collect(df[df['Month'] == input_month], f"Month: {input_month}")
        if {'Start_Weekday', '15Min'}.issubset(df.columns):
            analyze_and_collect(df[(df['Start_Weekday'] == input_weekday) & (df['15Min'] == time_input)], f"{input_weekday} @ {time_input} (15M)")
        if {'Start_Weekday', 'Hour'}.issubset(df.columns):
            analyze_and_collect(df[(df['Start_Weekday'] == input_weekday) & (df['Hour'] == input_hour)], f"{input_weekday} @ {input_hour} (H)")
        if {'Start_Weekday', 'Month'}.issubset(df.columns):
            analyze_and_collect(df[(df['Start_Weekday'] == input_weekday) & (df['Month'] == input_month)], f"{input_weekday} in {input_month}")
        if 'Criteria' in df.columns:
            analyze_and_collect(df[df['Criteria'] == criteria_input], f"Criteria: {criteria_input}")
        if {'Criteria', '15Min'}.issubset(df.columns):
            analyze_and_collect(df[(df['Criteria'] == criteria_input) & (df['15Min'] == time_input)], f"{criteria_input} @ {time_input} (15M)")
        if {'Criteria', 'Hour'}.issubset(df.columns):
            analyze_and_collect(df[(df['Criteria'] == criteria_input) & (df['Hour'] == input_hour)], f"{criteria_input} @ {input_hour} (H)" )
        if {'Criteria', 'Start_Weekday'}.issubset(df.columns):
            analyze_and_collect(df[(df['Criteria'] == criteria_input) & (df['Start_Weekday'] == input_weekday)], f"{criteria_input} @ {input_weekday}")
        if {'Criteria', 'Month'}.issubset(df.columns):
            analyze_and_collect(df[(df['Criteria'] == criteria_input) & (df['Month'] == input_month)], f"{criteria_input} in {input_month}")
        if {'15Min', 'Month'}.issubset(df.columns):
            analyze_and_collect(df[(df['15Min'] == time_input) & (df['Month'] == input_month)], f"{time_input} @ {input_month}")
        if{'Hour', 'Month'}.issubset(df.columns):
            analyze_and_collect(df[(df['Hour'] == input_hour) & (df['Month'] == input_month)], f"{input_hour} @ {input_month}")
    # --- Plotting ---
    if results:
        results_df = pd.DataFrame(results)

        # Reshape to long format for grouped bar chart
        melted_df = pd.melt(results_df, id_vars="Description",
                            value_vars=["Trade Count", "Win Rate", "Profit/Loss"],
                            var_name="Metric", value_name="Value")


        fig = go.Figure()

        # Common text style
        if not with_overall:
          results_df = results_df[1:]
        text_style = dict(size=14, family="Verdana", color="black")

        # Trade Count (scaled x10)
        fig.add_trace(go.Bar(
            x=results_df["Description"],
            y=results_df["Trade Count"] * 20,  # Scaled for visibility
            name="Trade Count (x10)",
            marker_color="blue",
            offsetgroup=0,
            yaxis="y",
            text=results_df["Trade Count"],
            textposition="outside",
            textfont=text_style
        ))

        # Profit/Loss
        fig.add_trace(go.Bar(
            x=results_df["Description"],
            y=results_df["Profit/Loss"],
            name="Profit/Loss",
            marker_color="green",
            offsetgroup=1,
            yaxis="y",
            text=results_df["Profit/Loss"].astype(int),
            textposition="outside",
            textfont=text_style
        ))

        # Win Rate (converted to percentage)
        fig.add_trace(go.Bar(
            x=results_df["Description"],
            y=results_df["Win Rate"],
            name="Win Rate (%)",
            marker_color="orange",
            offsetgroup=2,
            yaxis="y2",
            text=(results_df["Win Rate"]).astype(int).astype(str) + '%',
            textposition="outside",
            textfont=text_style
        ))

        # Layout
        fig.update_layout(
            title="Grouped Trade Metrics with Value Labels and Bold Text",
            xaxis=dict(title="Analysis Type", tickangle=-45),
            yaxis=dict(title="Trade Count (x10) / Profit-Loss"),
            yaxis2=dict(
                title="Win Rate (%)",
                overlaying="y",
                side="right",
                showgrid=False
            ),
            barmode="group",
            height=500,
            bargap=0.25
        )

        fig.show()
        return results_df
    else:
        print("No statistics available to plot.")

"""##Creating Train Data"""

# prompt: instead of taking input in Overall_Stats(df, time_input, date_input, criteria_input, with_overall=False) function i want to calculate result for all row in df and make a new df called train_data. where there will be 36 column of each row result. columns name will be like 15M_Trade Count, 15Min_Win Rate, 15M_Profit/Loss, Hour_Trade Count, Hour_Win_rate, Hour_Profit/Loss,.....

def calculate_metrics(group_df):
    total_trades = len(group_df)
    if total_trades == 0:
        return 0, 0, 0.0  # Trade Count, Profit/Loss, Win Rate

    total_pl = group_df['Profit/Loss'].sum()
    total_wins = (group_df['Profit/Loss'] > 0).sum()
    win_rate = total_wins / total_trades

    return total_trades, total_pl, win_rate

# prompt: get_train_data(df) : this function iterate through df data frame  and extract all possible feature combination (total trade, win rate, total profit/Loss) calling this function :get_feature_data(df, index, row) . however, this function doing same thing again and again fro example if  in data 50 row  hour value is 18:00 then it calculate trade count , win rate, and profit/loss for 50 time.. it's time consuming . but i want to use dynamic programming..first save all possible combination in a variable first. then iterate through df and extract data from the all possible cobination variable

def get_precomputed_stats(df):
    # Pre-calculate all possible feature combinations
    unique_15min = df['15Min'].unique()
    unique_hours = df['Hour'].unique()
    unique_weekdays = df['Start_Weekday'].unique()
    unique_months = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December'] #df['Month'].unique()
    unique_criteria = df['Criteria'].unique()


    # Store pre-calculated statistics in a dictionary for efficient lookup
    precomputed_stats = {}

    # Calculate stats for each individual feature value
    for time_15min in unique_15min:
        df_15min = df[df['15Min'] == time_15min]
        stats = calculate_metrics(df_15min)
        precomputed_stats[('15Min', time_15min)] = stats

    for hour in unique_hours:
        df_hour = df[df['Hour'] == hour]
        stats = calculate_metrics(df_hour)
        precomputed_stats[('Hour', hour)] = stats

    for weekday in unique_weekdays:
        df_weekday = df[df['Start_Weekday'] == weekday]
        stats = calculate_metrics(df_weekday)
        precomputed_stats[('Weekday', weekday)] = stats

    for month in unique_months:
        df_month = df[df['Month'] == month]
        stats = calculate_metrics(df_month)
        precomputed_stats[('Month', month)] = stats

    for criteria in unique_criteria:
        df_criteria = df[df['Criteria'] == criteria]
        stats = calculate_metrics(df_criteria)
        precomputed_stats[('Criteria', criteria)] = stats

    # Calculate stats for combined features
    for weekday in unique_weekdays:
        for time_15min in unique_15min:
            df_weekday_15m = df[(df['Start_Weekday'] == weekday) & (df['15Min'] == time_15min)]
            stats = calculate_metrics(df_weekday_15m)
            precomputed_stats[('Weekday_15Min', weekday, time_15min)] = stats

        for hour in unique_hours:
            df_weekday_hour = df[(df['Start_Weekday'] == weekday) & (df['Hour'] == hour)]
            stats = calculate_metrics(df_weekday_hour)
            precomputed_stats[('Weekday_Hour', weekday, hour)] = stats

        for month in unique_months:
            df_weekday_month = df[(df['Start_Weekday'] == weekday) & (df['Month'] == month)]
            stats = calculate_metrics(df_weekday_month)
            precomputed_stats[('Weekday_Month', weekday, month)] = stats

    for criteria in unique_criteria:
        for time_15min in unique_15min:
            df_criteria_15m = df[(df['Criteria'] == criteria) & (df['15Min'] == time_15min)]
            stats = calculate_metrics(df_criteria_15m)
            precomputed_stats[('Criteria_15Min', criteria, time_15min)] = stats

        for hour in unique_hours:
            df_criteria_hour = df[(df['Criteria'] == criteria) & (df['Hour'] == hour)]
            stats = calculate_metrics(df_criteria_hour)
            precomputed_stats[('Criteria_Hour', criteria, hour)] = stats

        for weekday in unique_weekdays:
            df_criteria_weekday = df[(df['Criteria'] == criteria) & (df['Start_Weekday'] == weekday)]
            stats = calculate_metrics(df_criteria_weekday)
            precomputed_stats[('Criteria_Weekday', criteria, weekday)] = stats

        for month in unique_months:
            df_criteria_month = df[(df['Criteria'] == criteria) & (df['Month'] == month)]
            stats = calculate_metrics(df_criteria_month)
            precomputed_stats[('Criteria_Month', criteria, month)] = stats

    for hour in unique_hours:
        for month in unique_months:
            df_hour_month = df[(df['Hour'] == hour) & (df['Month'] == month)]
            stats = calculate_metrics(df_hour_month)
            precomputed_stats[('Hour_Month', hour, month)] = stats

    for time_15min in unique_15min:
        for month in unique_months:
            df_15min_month = df[(df['15Min'] == time_15min) & (df['Month'] == month)]
            stats = calculate_metrics(df_15min_month)
            precomputed_stats[('15Min_Month', time_15min, month)] = stats

    return precomputed_stats

precomputed_stats = get_precomputed_stats(df)

# Save precomputed_stats
joblib.dump(precomputed_stats, 'precomputed_stats.pkl')
precomputed_stats = joblib.load('precomputed_stats.pkl')

def get_feature_data(p_state, index,row):
       # Retrieve stats for individual features
      row_data = {}
      time_15min = row['15Min']
      trade_count_15m, pl_15m, win_rate_15m = precomputed_stats[('15Min', time_15min)]
      row_data['15Min_Trade Count'] = trade_count_15m
      row_data['15Min_Win Rate'] = win_rate_15m
      row_data['15Min_Profit/Loss'] = pl_15m

      hour = row['Hour']
      trade_count_hour, pl_hour, win_rate_hour = precomputed_stats[('Hour', hour)]
      row_data['Hour_Trade Count'] = trade_count_hour
      row_data['Hour_Win Rate'] = win_rate_hour
      row_data['Hour_Profit/Loss'] = pl_hour

      weekday = row['Start_Weekday']
      trade_count_weekday, pl_weekday, win_rate_weekday = precomputed_stats[('Weekday', weekday)]
      row_data['Weekday_Trade Count'] = trade_count_weekday
      row_data['Weekday_Win Rate'] = win_rate_weekday
      row_data['Weekday_Profit/Loss'] = pl_weekday

      month = row['Month']
      trade_count_month, pl_month, win_rate_month = precomputed_stats[('Month', month)]
      row_data['Month_Trade Count'] = trade_count_month
      row_data['Month_Win Rate'] = win_rate_month
      row_data['Month_Profit/Loss'] = pl_month

      criteria = row['Criteria']
      trade_count_criteria, pl_criteria, win_rate_criteria = precomputed_stats[('Criteria', criteria)]
      row_data['Criteria_Trade Count'] = trade_count_criteria
      row_data['Criteria_Win Rate'] = win_rate_criteria
      row_data['Criteria_Profit/Loss'] = pl_criteria


      # Retrieve stats for combined features
      trade_count_weekday_15m, pl_weekday_15m, win_rate_weekday_15m = precomputed_stats[('Weekday_15Min', weekday, time_15min)]
      row_data['Weekday_15Min_Trade Count'] = trade_count_weekday_15m
      row_data['Weekday_15Min_Win Rate'] = win_rate_weekday_15m
      row_data['Weekday_15Min_Profit/Loss'] = pl_weekday_15m

      trade_count_weekday_hour, pl_weekday_hour, win_rate_weekday_hour = precomputed_stats[('Weekday_Hour', weekday, hour)]
      row_data['Weekday_Hour_Trade Count'] = trade_count_weekday_hour
      row_data['Weekday_Hour_Win Rate'] = win_rate_weekday_hour
      row_data['Weekday_Hour_Profit/Loss'] = pl_weekday_hour

      trade_count_weekday_month, pl_weekday_month, win_rate_weekday_month = precomputed_stats[('Weekday_Month', weekday, month)]
      row_data['Weekday_Month_Trade Count'] = trade_count_weekday_month
      row_data['Weekday_Month_Win Rate'] = win_rate_weekday_month
      row_data['Weekday_Month_Profit/Loss'] = pl_weekday_month

      trade_count_criteria_15m, pl_criteria_15m, win_rate_criteria_15m = precomputed_stats[('Criteria_15Min', criteria, time_15min)]
      row_data['Criteria_15Min_Trade Count'] = trade_count_criteria_15m
      row_data['Criteria_15Min_Win Rate'] = win_rate_criteria_15m
      row_data['Criteria_15Min_Profit/Loss'] = pl_criteria_15m

      trade_count_criteria_hour, pl_criteria_hour, win_rate_criteria_hour = precomputed_stats[('Criteria_Hour', criteria, hour)]
      row_data['Criteria_Hour_Trade Count'] = trade_count_criteria_hour
      row_data['Criteria_Hour_Win Rate'] = win_rate_criteria_hour
      row_data['Criteria_Hour_Profit/Loss'] = pl_criteria_hour

      trade_count_criteria_weekday, pl_criteria_weekday, win_rate_criteria_weekday = precomputed_stats[('Criteria_Weekday', criteria, weekday)]
      row_data['Criteria_Weekday_Trade Count'] = trade_count_criteria_weekday
      row_data['Criteria_Weekday_Win Rate'] = win_rate_criteria_weekday
      row_data['Criteria_Weekday_Profit/Loss'] = pl_criteria_weekday

      trade_count_criteria_month, pl_criteria_month, win_rate_criteria_month = precomputed_stats[('Criteria_Month', criteria, month)]
      row_data['Criteria_Month_Trade Count'] = trade_count_criteria_month
      row_data['Criteria_Month_Win Rate'] = win_rate_criteria_month
      row_data['Criteria_Month_Profit/Loss'] = pl_criteria_month

      trade_count_hour_month, pl_hour_month, win_rate_hour_month = precomputed_stats[('Hour_Month', hour, month)]
      row_data['Hour_Month_Trade Count'] = trade_count_hour_month
      row_data['Hour_Month_Win Rate'] = win_rate_hour_month
      row_data['Hour_Month_Profit/Loss'] = pl_hour_month

      trade_count_15min_month, pl_15min_month, win_rate_15min_month = precomputed_stats[('15Min_Month', time_15min, month)]
      row_data['15Min_Month_Trade Count'] = trade_count_15min_month
      row_data['15Min_Month_Win Rate'] = win_rate_15min_month
      row_data['15Min_Month_Profit/Loss'] = pl_15min_month

      return row_data

def get_train_data(df, precomputed_stats):
    # Prepare an empty DataFrame to store the results for each row
    train_data_list = []

    # Iterate through each row of the original DataFrame
    for index, row in df.iterrows():
        row_data = get_feature_data(precomputed_stats, index, row)
        train_data_list.append(row_data)

    # Create the new DataFrame
    train_data = pd.DataFrame(train_data_list)
    # train_data will now contain the calculated statistics for each row of the original df
    # You can merge this back with the original df if needed:
    # train_data = pd.concat([df.reset_index(drop=True), train_data], axis=1)
    return train_data
train_data = get_train_data(df, precomputed_stats)

train_data.columns

"""###Train Data Processing"""

def clean_train_data(train_data):
  train_data = train_data.drop(columns=['R', 'Profit', 'Loss', 'Fee', 'Trade Duration (hours)','Profit/Loss', 'CL', 'CW'])
# train_data = clean_train_data(train_data)

train_data.shape

def processing_train_data(df, train_data, R_threshold = 7):
# Select the desired columns from the original DataFrame
# columns_to_copy = ['R', 'Result', 'Profit', 'Loss', 'Fee', 'Trade Duration (hours)', ]
  df['Result_R'] = df.apply(
      lambda row: np.floor(min(row['R'], R_threshold)) if row['Result'] == 'W' else 1,
      axis=1
  )

  count_feature = ['CL', 'CW',]
  ema_features = ['15M_9C21','15M_21C50', '1H_12C26', '1H_13C34', '4H_20C50', '4H_50C200',]
  volume_features = [ '15M_volume_spike', '15M_volume_norm',
                      '1H_volume_spike', '1H_volume_norm',
                      '4H_volume_spike', '4H_volume_norm']

  features_column_to_copy = count_feature + ema_features + volume_features

  columns_to_copy = ['R', 'Result','Result_R', 'Profit', 'Loss', 'Fee', 'Trade Duration (hours)', ]
  # columns_to_copy.extend(features_column_to_copy)
  columns_to_copy.extend(count_feature)

  # print(columns_to_copy)

  # Copy these columns to the train_data DataFrame
  for col in columns_to_copy:
      if col in df.columns: # Check if the column exists in the original df
          train_data[col] = df[col]
      else:
          print(f"Warning: Column '{col}' not found in the original DataFrame.")

  # Convert 'Result' column to 0 or 1
  train_data['Result'] = train_data['Result'].apply(lambda x: 1 if x == 'W' else 0)
  return train_data

train_data = processing_train_data(df, train_data)

def save_read_train_data(train_data, name ='train_data'):
  train_data.to_excel('train_data.xlsx', index=False)
  train_data = pd.read_excel('train_data.xlsx', index_col=None)
  train_data = train_data.loc[:, ~train_data.columns.str.contains('^Unnamed')]
  return train_data

train_data = save_read_train_data(train_data)

def check_train_data(train_data):
  print(train_data.shape)
  print(train_data.columns)

# check_train_data(train_data)

# prompt: print dtypes of train_data

# train_data.info()

def convert_train_data_type(train_data):
    # Convert 'Result' column to 0 or 1
    for col in train_data.columns:
      try:
        train_data[col] = pd.to_numeric(train_data[col], errors='coerce') # Convert to numeric first
        train_data[col] = train_data[col].fillna(0).astype(int) # Fill NaN created by coerce and convert to int
      except ValueError:
        print(f"Could not convert column '{col}' to integer.")
        # Optionally handle columns that cannot be converted to int

    # Display the first few rows and the data types to verify
    # print(train_data.head())
    # train_data.dtypes
    return train_data

# train_data = convert_train_data_type(train_data)

def extract_ptarget_feature(train_data):
    prediction_target = train_data[['Result', 'Result_R']]
    # Features for the model will be the cleaned DataFrame without the target and original result/R columns
    features = train_data.drop(columns=['Result', 'Result_R'], errors='ignore')
    features_filtered = features.drop(columns=['R', 'Profit', 'Loss', 'Fee', 'Trade Duration (hours)','Profit/Loss'], errors='ignore')


    return features, features_filtered, prediction_target

features, features_filtered, prediction_target = extract_ptarget_feature(train_data)

def show_ptarget_feature(features, prediction_target):
    print(f"number of features: {features_filtered.shape}")
    print(f"Number of unique values in prediction target: {prediction_target['Result_R'].nunique()}")
    print("List of unique values in prediction target:", prediction_target['Result_R'].unique().tolist())
show_ptarget_feature(features, prediction_target)

"""##Data Split"""

from re import X
def split_data(features, prediction_target):
    # Split the data into training and testing sets
    # X contains the features, y contains the target variable
    # X_train_P, X_test_P, y_train, y_test = train_test_split(features, prediction_target, test_size=0.1, random_state=42, stratify=prediction_target)
    X_train_P, X_test_P, y_train, y_test = train_test_split( features, prediction_target, test_size=0.1, shuffle=False)

    X_train = X_train_P.drop(columns=['R', 'Profit', 'Loss', 'Fee', 'Trade Duration (hours)','Profit/Loss'], errors='ignore')
    X_test = X_test_P.drop(columns=['R', 'Profit', 'Loss', 'Fee', 'Trade Duration (hours)','Profit/Loss'], errors='ignore')

    y_train_m = y_train['Result_R']
    y_test_m = y_test['Result_R']

    y_train = y_train['Result']
    y_test = y_test['Result']
    return X_train, X_test, y_train, y_test, y_train_m, y_test_m, X_train_P, X_test_P

X_train, X_test, y_train, y_test, y_train_m, y_test_m, X_train_P, X_test_P= split_data(features, prediction_target)

# Save training_feature_cols
training_feature_cols = X_train.columns
joblib.dump(training_feature_cols, 'training_feature_cols.pkl')

training_feature_cols = joblib.load('training_feature_cols.pkl')

def show_split_data(X_train, X_test, y_train, y_test, y_train_m, y_test_m):
  print("Shape of X_train:", X_train.shape)
  print("Shape of X_test:", X_test.shape)
  print("Shape of y_train:", y_train.shape)
  print("Shape of y_test:", y_test.shape)
  print("Shape of y_train_m:", y_train_m.shape)
  print("Shape of y_test_m:", y_test_m.shape)
show_split_data(X_train, X_test, y_train, y_test, y_train_m, y_test_m)

def show_model_result(model_name, accuracy, class_report):
    print(f"\nModel: {model_name}")
    print(f"Accuracy: {accuracy:.4f}")
    print("\nClassification Report:")
    print(class_report)

"""##Random Forest"""

def RF(X_train, X_test, y_train, y_test, n_estimators=100):
    RF_model = RandomForestClassifier(n_estimators=n_estimators, random_state=42)

    start_time = time.time()
    RF_model.fit(X_train, y_train)
    end_time = time.time()
    # print(f"Training time: {end_time - start_time:.2f} seconds")
    importances = RF_model.feature_importances_
    features_df = pd.DataFrame({
        'Feature': X_train.columns,
        'Importance': importances
    }).sort_values(by='Importance', ascending=False)

    # Plot
    # features_df.plot.bar(x='Feature', y='Importance', figsize=(12, 4), title='Feature Importances')
    # plt.tight_layout()
    # plt.show()

    # Make predictions on the test set
    start_time = time.time()
    y_pred_RF= RF_model.predict(X_test)
    end_time = time.time()
    # print(f"Prediction time: {end_time - start_time:.2f} seconds")

    # Evaluate the model
    RF_accuracy = accuracy_score(y_test, y_pred_RF)
    RF_class_report = classification_report(y_test, y_pred_RF)
    return RF_model,y_pred_RF, RF_accuracy, RF_class_report

RF_model,y_pred_RF, RF_accuracy, RF_class_report = RF(X_train, X_test, y_train, y_test, n_estimators=100)
show_model_result("Random Forest", RF_accuracy, RF_class_report)

"""##XVG Boost"""

def XVG(X_train, X_test, y_train, y_test, eval_metric='loggloss'):
    XVG_model = XGBClassifier(eval_metric=eval_metric)
    XVG_model.fit(X_train, y_train)
    importances = XVG_model.feature_importances_
    features_df = pd.DataFrame({
        'Feature': X_train.columns,
        'Importance': importances
    }).sort_values(by='Importance', ascending=False)

    # Plot
    # features_df.plot.bar(x='Feature', y='Importance', figsize=(12, 4), title='Feature Importances')
    # plt.tight_layout()
    # plt.show()

    y_pred_XVG = XVG_model.predict(X_test)
    XVG_accuracy = accuracy_score(y_test, y_pred_XVG)
    XVG_class_report = classification_report(y_test, y_pred_XVG)
    return XVG_model,y_pred_XVG, XVG_accuracy, XVG_class_report

XVG_model,y_pred_XVG, XVG_accuracy, XVG_class_report = XVG(X_train, X_test, y_train, y_test, eval_metric='logloss')
show_model_result("XGBoost", XVG_accuracy, XVG_class_report)

"""##Logistic Regression"""

def LR(X_train, X_test, y_train, y_test, solver='lbfgs', max_iter=10000):
# Logistic Regression (with scaling
# Pipeline: Scaling + Logistic Regression
    LR_model = make_pipeline(StandardScaler(), LogisticRegression(solver=solver, max_iter=max_iter,random_state=42))
    LR_model.fit(X_train, y_train)


    # estimator = LogisticRegression(random_state=42, max_iter=1000)

    # pipe = Pipeline([
    #     ('scaler', StandardScaler()),
    #     ('lr', estimator)
    # ])


    # lr_pipeline = RFE(estimator=estimator, n_features_to_select=30)
    # lr_pipeline.fit(StandardScaler().fit_transform(X_train), y_train)  # manually scale here for RFE

    # selected_features = X_train.columns[lr_pipeline.support_]
    # ranking = lr_pipeline.ranking_

    # feature_ranking = pd.DataFrame({
    #     'Feature': X_train.columns,
    #     'Rank': ranking,
    #     'Selected': lr_pipeline.support_
    # }).sort_values(by='Rank')

    # print("Top Selected Features:")
    # print(selected_features.tolist())
    # print("\nFull Feature Ranking:\n", feature_ranking)

    y_pred_LR = LR_model.predict(X_test)
    LR_accuracy = accuracy_score(y_test, y_pred_LR)
    LR_class_report = classification_report(y_test, y_pred_LR)
    return LR_model,y_pred_LR, LR_accuracy, LR_class_report

LR_model,y_pred_LR, LR_accuracy, LR_class_report = LR(X_train, X_test, y_train, y_test, solver='lbfgs', max_iter=10000)
show_model_result("Logistic Regression", LR_accuracy, LR_class_report)

"""##Voting Classifier"""

def VC(X_train, X_test, y_train, y_test, max_iter= 1000, eval_metric='logloss', voting='soft', ul_enc=False):
    lr_p1= make_pipeline(StandardScaler(), LogisticRegression(max_iter=max_iter, random_state=42))
    VC_model = VotingClassifier(
        estimators=[
            ('lr', lr_p1),
            ('rf', RandomForestClassifier(random_state=42)),
            ('xgb', XGBClassifier(eval_metric=eval_metric, use_label_encoder=ul_enc, random_state=42))
        ],
        voting=voting  # use 'hard' for majority vote or 'soft' for averaged probabilities
    )

    VC_model.fit(X_train, y_train)
    y_pred_VC = VC_model.predict(X_test)

    VC_accuracy = accuracy_score(y_test, y_pred_VC)
    VC_class_report = classification_report(y_test, y_pred_VC)
    return VC_model,y_pred_VC, VC_accuracy, VC_class_report

VC_model,y_pred_VC, VC_accuracy, VC_class_report = VC(X_train, X_test, y_train, y_test, max_iter= 1000, eval_metric='logloss', voting='soft', ul_enc=False)
show_model_result("Voting Classifier", VC_accuracy, VC_class_report)

"""##Stacking Classifier"""

def S(X_train, X_test, y_train, y_test, max_iter= 1000, eval_metric='logloss', cv =5, n_jobs=-1, ul_enc=False):

    lr_p2 = make_pipeline(StandardScaler(), LogisticRegression(max_iter=max_iter, random_state=42))
    base_models = [
        ('lr', lr_p2),
        ('rf', RandomForestClassifier(random_state=42)),
        ('xgb', XGBClassifier(eval_metric=eval_metric, use_label_encoder=ul_enc, random_state=42))
    ]
    final_estimator = make_pipeline(
        StandardScaler(),
        LogisticRegression(max_iter=max_iter, random_state=42)
    )
    S_model = StackingClassifier(
        estimators=base_models,
        final_estimator=final_estimator,  # You can also use XGBClassifier here
        passthrough=True,  # Optional: pass original features to meta-model
        cv=cv,
        n_jobs=n_jobs
    )

    S_model.fit(X_train, y_train)
    y_pred_S = S_model.predict(X_test)

    S_accuracy = accuracy_score(y_test, y_pred_S)
    S_class_report = classification_report(y_test, y_pred_S)
    return S_model, y_pred_S, S_accuracy, S_class_report

S_model,y_pred_S, S_accuracy, S_class_report = S(X_train, X_test, y_train, y_test, max_iter= 1000, eval_metric='logloss', cv=5, n_jobs=-1, ul_enc=False)
show_model_result("Stacking Classifier", S_accuracy, S_class_report)

"""#Model Result Analysis & Visualization"""

# prompt: confustion matric for 3 model and show them with accuracy show graph horizontally not vetically

def plot_confusion_matrices_horizontal(y_true, predictions, model_names, ticks= ['LOSS', 'WIN']):
    """
    Plots confusion matrices for multiple models horizontally with accuracy scores.

    Args:
        y_true (array-like): True labels.
        predictions (list of array-like): List of prediction arrays from each model.
        model_names (list of str): List of names for each model.
    """

    num_models = len(predictions)
    if num_models == 0 or len(predictions) != len(model_names):
        print("Invalid input for plotting confusion matrices.")
        return

    fig, axes = plt.subplots(2, int(num_models/2)+1, figsize=(5 * int(num_models/2) + 1, 6))
    if num_models == 1:
        axes = [axes] # Ensure axes is iterable even for one model
    r = 0
    for i, y_pred in enumerate(predictions):
        p_txt=""
        t_txt=""
        if i == 3:
          r =1
        if len(ticks) >2:
          ticks_u = np.union1d(y_true, y_pred)
          diff = np.setdiff1d(ticks , ticks_u)
          if len(ticks_u) != 0:
            p_txt = f"Missingn Classes: {np.setdiff1d(ticks, y_pred).tolist()}"
            t_txt = f"Missing Classes: {np.setdiff1d(ticks, y_true).tolist()}"
            ticks = ticks_u


        cm = confusion_matrix(y_true, y_pred)
        accuracy = accuracy_score(y_true, y_pred)
        sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", ax=axes[r][i%3], cbar=False)
        axes[r, i%3].set_title(f'{model_names[i]}\nAccuracy: {accuracy:.2f}', fontsize=11)
        axes[r, i%3].set_xlabel(f'Predicted Label {p_txt}', fontsize=10)
        axes[r, i%3].set_ylabel(f'True Label {t_txt}', fontsize=10)
        axes[r, i%3].set_xticklabels(ticks)
        axes[r, i%3].set_yticklabels(ticks)

    plt.tight_layout()
    plt.show()

# List of predictions from the trained models
all_predictions = [y_pred_RF, y_pred_XVG, y_pred_LR, y_pred_VC, y_pred_S]

# List of model names
model_names = ['RandomForest', 'XGBoost', 'Logistic Regression', 'Voting Classifier', 'Stacking Classifier']

# Plot the confusion matrices horizontally
plot_confusion_matrices_horizontal(y_test, all_predictions, model_names)

"""##Get Train Data From Input"""

# prompt: get_train_data_for_input(df, time_input, date_input, criteria_input): instead of this get_train_data_for_input_optimized(precompute_stats, time_input, date_input, criteria_input) so that it can take data from precompute state instead of df

def get_train_data_for_input(precomputed_stats, time_input, date_input, criteria_input, training_feature_cols=[]):

    # Convert inputs
    try:
        input_time = datetime.strptime(time_input, '%H:%M').time()
    except ValueError:
        print("Invalid time format. Use HH:MM.")
        return pd.DataFrame()

    try:
        input_date = datetime.strptime(date_input, '%m/%d/%Y')
    except ValueError:
        print("Invalid date format. Use M/D/YYYY.")
        return pd.DataFrame()

    row_data = {}
    input_datetime = datetime.combine(input_date, input_time)

    time_15min = input_datetime.strftime('%H:%M')
    hour = input_datetime.hour
    weekday = input_datetime.strftime('%A')
    month = input_datetime.strftime('%B')
    criteria = criteria_input

    # Retrieve stats from precomputed_stats dictionary
    try:
        row = {'15Min': time_15min, 'Hour': hour, 'Start_Weekday': weekday, 'Month': month, 'Criteria': criteria}
        row_data = get_feature_data(precomputed_stats, 0, row)

    except KeyError as e:
        print(f"Error: Missing precomputed statistic for key {e}. Ensure create_precomputed_stats covers all combinations you need.")
        return pd.DataFrame()


    # Create DataFrame from the calculated row_data
    input_features_df = pd.DataFrame([row_data])
    # Reindex the input_features_df to match the training columns, filling missing with 0
    input_features_df = input_features_df.reindex(columns=training_feature_cols, fill_value=0)

    # Convert all columns to the same data type as the training features (assuming int from previous steps)
    for col in input_features_df.columns:
        try:
             input_features_df[col] = pd.to_numeric(input_features_df[col], errors='coerce')
             input_features_df[col] = input_features_df[col].fillna(0).astype(int)
        except ValueError:
            print(f"Warning: Could not convert input column '{col}' to integer.")


    return input_features_df

# Example usage (assuming X_train is defined from the previous code):
# Get the column names from your training features DataFrame
training_feature_cols = X_train.columns.tolist()

# Call the optimized function
# input_data = get_train_data_for_input_optimized(precomputed_stats, '09:30', '01/8/2023', 'ELC', training_feature_cols)
# print(input_data)

# --- Prediction using the trained model ---
def predict_trade_result(model, input_features_df):
    if input_features_df.empty:
        print("Cannot predict: Invalid input features.")
        return None, None

    # Make prediction
    predicted_class = model.predict(input_features_df)[0]
    if predicted_class == 0:
        predicted_proba = model.predict_proba(input_features_df)
    else:
        predicted_proba = model.predict_proba(input_features_df)

    return predicted_class, predicted_proba

"""##Overall Statistic Using Model Prediction"""

def overall_stats(df, features_filtered):
    overall_stats_df = pd.DataFrame()

    models = {
        'RandomForest': RF_model,
        'XGBoost': XVG_model,
        'Logistic Regression': LR_model,
        'Voting Classifier': VC_model,
        'Stacking Classifier': S_model
    }

    # Ensure required columns for filtering exist in the original df
    if 'Profit/Loss' not in df.columns:
        df['Profit/Loss'] = df['Profit'] - df['Loss'] - df['Fee']
    if 'Trade Duration (hours)' not in df.columns:
        df['Trade Duration (hours)'] = (df['exitDateTime'] - df['startDateTime']).dt.total_seconds() / 3600


    for model_name, model in models.items():

        # 1. Calculate pred result using feature
        # Predict on the entire feature set (features DataFrame)
        pred_result = model.predict(features_filtered)

        # 2. Use that result to filter df for took trade
        # Create a new column based on the prediction
        # We only consider trades where the model predicted a "Win" (class 1)
        df_filtered = df.copy()
        df_filtered['Predicted_Result'] = pred_result
        df_took_trade = df_filtered[df_filtered['Predicted_Result'] == 1]
        # print(df_took_trade.head( ))
        # 3. Overall statistic for filtered data frame
        if not df_took_trade.empty:
            total_trades = len(df_took_trade)
            total_wins = (df_took_trade['Profit/Loss'] > 0).sum()
            win_rate = total_wins / total_trades
            total_profit = df_took_trade['Profit'].sum()
            total_loss = df_took_trade['Loss'].sum()
            total_fee = df_took_trade['Fee'].sum()
            max_profit = df_took_trade['Profit/Loss'].max()
            max_loss = df_took_trade['Profit/Loss'].min()
            avg_profit = df_took_trade[df_took_trade['Profit/Loss'] > 0]['Profit/Loss'].mean() if (df_took_trade['Profit/Loss'] > 0).sum() > 0 else 0
            avg_loss = df_took_trade[df_took_trade['Profit/Loss'] < 0]['Profit/Loss'].mean() if (df_took_trade['Profit/Loss'] < 0).sum() > 0 else 0
            total_time = df_took_trade['Trade Duration (hours)'].sum()
            average_time = df_took_trade['Trade Duration (hours)'].mean()
            realized_pl = df_took_trade['Profit/Loss'].sum()
            avg_r_positive_pl = df_took_trade[df_took_trade['Profit/Loss'] > 0]['R'].mean()
            # Append the statistics as a new row to the overall_stats_df
            model_stats = {
                'Model': model_name,
                'Total Trades': total_trades,
                'Total Win Count': total_wins,
                'Total Loss Count': (df_took_trade['Profit/Loss'] < 0).sum(),
                'Total Profit': total_profit,
                'Total Loss': total_loss,
                'Total Fee': total_fee,
                'Win Rate (%)': round(win_rate * 100, 2),
                'Average R': round(avg_r_positive_pl, 2),
                'Max Profit': round(max_profit, 2),
                'Average Profit': round(avg_profit, 2),
                'Average Loss': round(avg_loss, 2),
                'Total Time (hours)': round(total_time, 2),
                'Average Time (hours)': round(average_time, 2),
                'Realized Profit/Loss': round(realized_pl, 2),
                'R': round(total_profit / (total_loss + total_fee), 2)
            }

            overall_stats_df = pd.concat([overall_stats_df, pd.DataFrame([model_stats])], ignore_index=True)

        else:
            print(f"No trades were 'taken' based on {model_name} predictions.")
            # Add a row of zeros for models with no predicted 'Win' trades
            model_stats = {
                'Model': model_name,
                'Total Trades': 0,
                'Total Win Count': 0,
                'Total Loss Count': 0,
                'Total Profit': 0,
                'Total Loss': 0,
                'Total Fee': 0,
                'Win Rate (%)': 0.0,
                'Max Profit': 0.0,
                'Average Profit': 0.0,
                'Average Loss': 0.0,
                'Total Time (hours)': 0.0,
                'Average Time (hours)': 0.0,
                'Realized Profit/Loss': 0.0
            }
            overall_stats_df = pd.concat([overall_stats_df, pd.DataFrame([model_stats])], ignore_index=True)
    return overall_stats_df

# Display the final overall statistics DataFrame
# overall_stats_df = overall_stats(df, features_filtered)
overall_stats_df = overall_stats(X_test_P, X_test)
print("\nOverall Statistics for Trades Predicted as 'Win' by Each Model:")
overall_stats_df

time_input = "20:00"  #@param {type:"string"}
date_input = "6/25/2025"  #@param {type:"string"}
criteria_input = "LG RGC UP"  #@param {type:"string"}
CW = 0 #@param {type:"integer"}
CL = 4 #@param {type:"integer"}

result = Overall_R(df, time_input, date_input, criteria_input, with_overall=False)
input_features = get_train_data_for_input(precomputed_stats, time_input, date_input, criteria_input, training_feature_cols)
# Predict the result
input_features['CL'] = [CL]
input_features['CW'] = [CW]
# print(input_features.head())
if not input_features.empty:
    # Get predictions and probabilities for each model
    rf_predicted_class, rf_predicted_probability = predict_trade_result(RF_model, input_features)
    xgb_predicted_class, xgb_predicted_probability = predict_trade_result(XVG_model, input_features)
    lr_predicted_class, lr_predicted_probability = predict_trade_result(LR_model, input_features)
    VC_model_predicted_class, VC_model_predicted_probability = predict_trade_result(VC_model, input_features)
    S_model_predicted_class, S_model_predicted_probability = predict_trade_result(S_model, input_features)

    print(f"\n--- Predictions for Date: {date_input}, Time: {time_input}, Criteria: {criteria_input} ---")

    print("\nRandomForest Model:")
    print(f"  Test Set Accuracy: {RF_accuracy:.4f}")
    print(f"  Predicted Result: {'Win' if rf_predicted_class == 1 else 'Loss'}")
    print(f"  Predicted Probability of {'Win' if rf_predicted_class == 1 else 'Loss'}: {np.max(rf_predicted_probability):.4f}")

    print("\nXGBoost Model:")
    print(f"  Test Set Accuracy: {XVG_accuracy:.4f}")
    print(f"  Predicted Result: {'Win' if xgb_predicted_class == 1 else 'Loss'}")
    print(f"  Predicted Probability of {'Win' if rf_predicted_class == 1 else 'Loss'}: {np.max(xgb_predicted_probability):.4f}")

    print("\nLogistic Regression Model:")
    print(f"  Test Set Accuracy: {LR_accuracy:.4f}")
    print(f"  Predicted Result: {'Win' if lr_predicted_class == 1 else 'Loss'}")
    print(f"  Predicted Probability of {'Win' if rf_predicted_class == 1 else 'Loss'}: {np.max(lr_predicted_probability):.4f}")

    print("\nVoting Classifier Model:")
    print(f"  Test Set Accuracy: {VC_accuracy:.4f}")
    print(f"  Predicted Result: {'Win' if VC_model_predicted_class == 1 else 'Loss'}")
    print(f"  Predicted Probability of {'Win' if VC_model_predicted_class == 1 else 'Loss'}: {np.max(VC_model_predicted_probability):.4f}")

    print("\nStacking Classifier Model:")
    print(f"  Test Set Accuracy: {S_accuracy:.4f}")
    print(f"  Predicted Result: {'Win' if S_model_predicted_class == 1 else 'Loss'}")
    print(f"  Predicted Probability of {'Win' if S_model_predicted_class == 1 else 'Loss'}: {np.max(S_model_predicted_probability):.4f}")

else:
    print("Prediction could not be made due to invalid inputs or data.")

"""##MultiClass Model

##Multiclass - Average R

###Label Encode
"""

def encode_train_test(y_train_m, y_test_m):
    label_encoder = LabelEncoder()
    y_train_enc = label_encoder.fit_transform(y_train_m)
    y_test_enc = label_encoder.transform(y_test_m)

    # Save the mapping for later decoding
    class_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))
    reverse_mapping = dict(zip(label_encoder.transform(label_encoder.classes_), label_encoder.classes_))
    return label_encoder, y_train_enc, y_test_enc, class_mapping, reverse_mapping
label_encoder, y_train_enc, y_test_enc, class_mapping, reverse_mapping = encode_train_test(y_train_m, y_test_m)

"""###Random Forest"""

# prompt: its a trading data analysis .. 36 input and result in  binary 1 or 0 .. choose best machine learning model and train then test and show the outcome


def RF_M(X_train, y_train_enc, X_test, y_test_enc, n_estimators= 100):
    # Initialize and train the model
    # RandomForestClassifier is a good choice for binary classification and handles various feature types well.
    # It's relatively robust to outliers and doesn't require extensive feature scaling.
    # The number of estimators (n_estimators) can be tuned. More trees generally improve performance but increase computation time.
    # random_state for reproducibility
    RF_model_m = RandomForestClassifier(n_estimators=n_estimators, random_state=42)

    start_time = time.time()
    RF_model_m.fit(X_train, y_train_enc)
    end_time = time.time()
    # print(f"Training time: {end_time - start_time:.2f} seconds")
    importances = RF_model_m.feature_importances_
    features_df = pd.DataFrame({
        'Feature': X_train.columns,
        'Importance': importances
    }).sort_values(by='Importance', ascending=False)

    # Plot
    # features_df.plot.bar(x='Feature', y='Importance', figsize=(12, 4), title='Feature Importances')
    # plt.tight_layout()
    # plt.show()

    # Make predictions on the test set
    start_time = time.time()
    y_pred_RF_m= RF_model_m.predict(X_test)
    end_time = time.time()
    # print(f"Prediction time: {end_time - start_time:.2f} seconds")

    # Evaluate the model
    RF_accuracy_m = accuracy_score(y_test_enc, y_pred_RF_m)
    RF_class_report_m = classification_report(y_test_enc, y_pred_RF_m)
    return RF_model_m, y_pred_RF_m, RF_accuracy_m, RF_class_report_m

RF_model_m, y_pred_RF_m, RF_accuracy_m, RF_class_report_m = RF_M(X_train, y_train_enc, X_test, y_test_enc, n_estimators=100)
show_model_result("RandomForest Multi", RF_accuracy_m, RF_class_report_m)

"""###XVG Boost"""

def XVG_M(X_train, y_train_enc, X_test, y_test_enc,num_class=5, objective='multi:softmax', eval_metric = 'mlogloss', ul_enc = False):
    # Initialize and train the model
    XVG_model_m = XGBClassifier(
        objective=objective,   # or 'multi:softprob' for probabilities
        num_class=num_class,
        eval_metric=eval_metric,      # Recommended for multi-class
        use_label_encoder=ul_enc,     # Suppress warnings
        random_state=42
    )

    XVG_model_m.fit(X_train, y_train_enc)
    y_pred_XVG_m = XVG_model_m.predict(X_test)

    print("Test class counts:\n", pd.Series(y_test_enc).value_counts())
    print("Predicted class counts:\n", pd.Series(y_pred_XVG_m).value_counts())

    XVG_accuracy_m = accuracy_score(y_test_enc, y_pred_XVG_m)
    XVG_class_report_m = classification_report(y_test_enc, y_pred_XVG_m)
    return XVG_model_m, y_pred_XVG_m, XVG_accuracy_m, XVG_class_report_m

XVG_model_m, y_pred_XVG_m, XVG_accuracy_m, XVG_class_report_m = XVG_M(X_train, y_train_enc, X_test, y_test_enc, num_class=prediction_target['Result_R'].nunique(), objective='multi:softmax', eval_metric = 'mlogloss', ul_enc = False)
show_model_result("XGBoost Multi", XVG_accuracy_m, XVG_class_report_m)

"""###Logistic Regression"""

def LR_M(X_train, y_train_enc, X_test, y_test_enc, solver='lbfgs', max_iter=10000):

    # Pipeline: Scaling + Logistic Regression
    # lr_model_m = LogisticRegression(solver='lbfgs', max_iter=10000, class_weight='balanced', )
    lr_model_m = LogisticRegression(solver=solver, max_iter=max_iter,random_state=42 )
    LR_model_m = make_pipeline(StandardScaler(), lr_model_m)
    # model = RandomForestClassifier()
    # model = XGBClassifier(num_class=5, objective='multi:softmax')

    LR_model_m.fit(X_train, y_train_enc)
    y_pred_LR_m = LR_model_m.predict(X_test)

    LR_accuracy_m = accuracy_score(y_test_enc, y_pred_LR_m)
    LR_class_report_m = classification_report(y_test_enc, y_pred_LR_m)

    return LR_model_m, y_pred_LR_m, LR_accuracy_m, LR_class_report_m

LR_model_m, y_pred_LR_m, LR_accuracy_m, LR_class_report_m = LR_M(X_train, y_train_enc, X_test, y_test_enc, solver='lbfgs', max_iter=10000)
show_model_result("Logistic Regression Multi", LR_accuracy_m, LR_class_report_m)

"""###Voting Classifier"""

def VC_M(X_train, y_train_enc, X_test, y_test_enc, num_class=5, voting='soft', solver='lbfgs', max_iter=10000, objective='multi:softmax',  eval_metric='mlogloss', ul_enc=False):
    lr = make_pipeline(StandardScaler(), LogisticRegression(solver=solver, max_iter=max_iter, random_state=42 )) #class_weight='balanced',

    rf = RandomForestClassifier(random_state=42)

    xgb = XGBClassifier(
        objective=objective,
        num_class=num_class,
        eval_metric=eval_metric,
        use_label_encoder=ul_enc,
        random_state=42
    )

    # Check all estimators before using
    # for name, model in [('lr', lr), ('rf', rf), ('xgb', xgb)]:
    #     model.fit(X_train, y_train_enc)
    #     proba = model.predict_proba(X_test)
    #     print(f"{name} proba shape: {proba.shape}")

    VC_model_m = VotingClassifier(
        estimators=[
            ('lr', lr),
            ('rf', rf),
            ('xgb', xgb)
        ],
        voting=voting # Use 'soft' for probability averaging (recommended for multi-class)
    )

    VC_model_m.fit(X_train, y_train_enc)
    y_pred_VC_m = VC_model_m.predict(X_test)

    VC_accuracy_m = accuracy_score(y_test_enc, y_pred_VC_m)
    VC_class_report_m = classification_report(y_test_enc, y_pred_VC_m)

    return VC_model_m, y_pred_VC_m, VC_accuracy_m, VC_class_report_m
print(X_train.shape, y_train_enc.shape, X_test.shape, y_test_enc.shape)
VC_model_m, y_pred_VC_m, VC_accuracy_m, VC_class_report_m = VC_M(X_train, y_train_enc, X_test, y_test_enc, num_class=prediction_target['Result_R'].nunique(), voting='soft', solver='lbfgs', max_iter=1000, objective='multi:softmax', eval_metric='mlogloss', ul_enc=False)
show_model_result("Voting Classifier Multi", VC_accuracy_m, VC_class_report_m)

"""###Stacking Classifier"""

def S_M(X_train, y_train_enc, X_test, y_test_enc, num_class=5, solver='lbfgs', max_iter=10000, objective='multi:softmax', eval_metric='mlogloss',cv =5, n_jobs=-1, ul_enc=False):

    lr_p2_m = make_pipeline(StandardScaler(), LogisticRegression(solver=solver, max_iter=max_iter, random_state=42 )) #class_weight='balanced',
    base_models = [
        ('lr', lr_p2_m),
        ('rf', RandomForestClassifier(random_state=42)),
        ('xgb', XGBClassifier(objective=objective, num_class=num_class, eval_metric=eval_metric, use_label_encoder=ul_enc,))
    ]
    final_estimator = make_pipeline(
        StandardScaler(),
        LogisticRegression(max_iter=max_iter, random_state=42, ) #class_weight='balanced',
    )
    S_model_m = StackingClassifier(
        estimators=base_models,
        final_estimator=final_estimator,
        passthrough=True,
        cv=cv,
        n_jobs=n_jobs
    )

    S_model_m .fit(X_train, y_train_enc)
    y_pred_S_m = S_model_m .predict(X_test)

    S_accuracy_m = accuracy_score(y_test_enc, y_pred_S_m)
    S_class_report_m = classification_report(y_test_enc, y_pred_S_m)
    return S_model_m, y_pred_S_m, S_accuracy_m, S_class_report_m

S_model_m, y_pred_S_m, S_accuracy_m, S_class_report_m = S_M(X_train, y_train_enc, X_test, y_test_enc,num_class=prediction_target['Result_R'].nunique(), solver='lbfgs', max_iter=10000, objective='multi:softmax', eval_metric='mlogloss',cv =5, n_jobs=-1, ul_enc=False)
show_model_result("Stacking Classifier Multi", S_accuracy_m, S_class_report_m)

# List of predictions from the trained models
all_predictions = [y_pred_RF_m, y_pred_XVG_m, y_pred_LR_m, y_pred_VC_m, y_pred_S_m]

# List of model names
model_names = ['RandomForest', 'XGBoost', 'Logistic Regression', 'Voting Classifier', 'Stacking Classifier']

# Plot the confusion matrices horizontally
plot_confusion_matrices_horizontal(y_test, all_predictions, model_names,ticks = sorted(np.unique(y_test_enc)))

"""#Showing Result"""

# pip install tabulate if needed

def show_predictions(time_input, date_input, criteria_input, CW, CL, training_feature_cols, model_names, model_keys,b_models=None, m_models=None, b_acc=None, m_acc=None, Max_R=False, w_threshold=2):
    mr = "_max_r" if Max_R else ""
    # Initialize models and their keys
    binary_models = b_models if b_models is not None else [globals()[f"{k}_model{mr}"] for k in model_keys]
    binary_accuracies = b_acc if b_acc is not None else [globals()[f"{k}_accuracy{mr}"] for k in model_keys]

    multiclass_models = m_models if m_models is not None else [globals()[f"{k}_model_m{mr}"] for k in model_keys]
    multiclass_accuracies = m_acc if m_acc is not None else [globals()[f"{k}_accuracy_m{mr}"] for k in model_keys]

    input_features = get_train_data_for_input(precomputed_stats, time_input, date_input, criteria_input, training_feature_cols)
    input_features['CL'] = [CL]
    input_features['CW'] = [CW]
    predictions = []
    predictions_multi = []
    if not input_features.empty:
        table_data = []

        for i, name in enumerate(model_names):
            # Predict binary
            pred_bin, prob_bin = predict_trade_result(binary_models[i], input_features)
            label_bin = 'Win' if pred_bin == 1 else 'Loss'
            acc_bin = f"{binary_accuracies[i]:.2f}"
            bin_str = f"{label_bin} ({np.max(prob_bin):.2f}), Acc: {acc_bin}"

            # Predict multiclass
            pred_multi, prob_multi = predict_trade_result(multiclass_models[i], input_features)
            acc_multi = f"{multiclass_accuracies[i]:.2f}"
            multi_str = f"Class {label_encoder.inverse_transform([pred_multi])[0]} ({np.max(prob_multi):.2f}), Acc: {acc_multi}"
            table_data.append([name, bin_str, multi_str])
            predictions.append(pred_bin)
            predictions_multi.append(pred_multi)
        predicted_win = predictions.count(1)
        predicted_loss = predictions.count(0)
        predicted_win_multi = len(predictions)-predictions_multi.count(0)
        predicted_loss_multi = predictions_multi.count(0)
        trade_decision = "Win" if predicted_win > w_threshold else "Loss"
        trade_decision_multi = "Win" if predicted_win_multi >= w_threshold else "Loss"
        max_r = "" if trade_decision_multi == "Loss" else "- Max R: "+str(np.max(predictions_multi))
        avg_r = "" if trade_decision_multi == "Loss" else "- Average R:"+str(np.mean([x for x in predictions_multi if x>0]))
        # Display table
        print(f"\n--- 📊 Combined Model Predictions for {date_input} {time_input} ({criteria_input}) -{mr} ---\n")
        print(f"Overall decision: ---- Binary: ** {trade_decision} **  Multi: ** {trade_decision_multi}** ---- {max_r} {avg_r}")
        print(f"Binary Models: Win Predicted **{predicted_win} time **    Multi Models: Win Predicted **{predicted_win_multi}**")
        print(tabulate(table_data, headers=["Model", "📘 Binary (Win/Loss)", "📗 Multi-Class (R Bucket)"], tablefmt="fancy_grid"))

    else:
        print("❌ Prediction could not be made due to invalid inputs or data.")

"""###Result View"""

time_input = "17:00"  #@param {type:"string"}
date_input = "6/26/2025"  #@param {type:"string"}
criteria_input = "ELC"  #@param {type:"string"}
CW = 0 #@param {type:"integer"}
CL = 6 #@param {type:"integer"}
# Define model base names
model_names = ['RandomForest', 'XGBoost', 'Logistic Regression', 'Voting Classifier', 'Stacking Classifier']
model_keys = ['RF', 'XVG', 'LR', 'VC', 'S']

show_predictions(time_input, date_input, criteria_input, CW, CL,training_feature_cols, model_names, model_keys)
# List of predictions from the trained models
all_predictions = [y_pred_RF, y_pred_XVG, y_pred_LR, y_pred_VC, y_pred_S]

# Plot the confusion matrices horizontally
# plot_confusion_matrices_horizontal(y_test, all_predictions, model_names)
# result = Overall_R(df, time_input, date_input, criteria_input, with_overall=False)

# Display the final overall statistics DataFrame
# overall_stats_df = overall_stats(df, features_filtered)
overall_stats_df = overall_stats(X_test_P, X_test)
print("\nOverall Statistics for Trades Predicted as 'Win' by Each Model:")
overall_stats_df

# prompt: based on binary models prediction result put only trades that signal win and then based on filtered data index give me the filtered result of df so that i can get all column.. i want to download 5 excel sheet for 5 binary models prediciton result

# Function to filter the original DataFrame based on binary model predictions
def filter_trades_by_prediction(df, features, model):
  """
    Filters the original DataFrame to include only trades where the given
    binary model predicted a 'Win' (class 1).

    Args:
        df (pd.DataFrame): The original DataFrame containing all trade data.
        features (pd.DataFrame): The feature DataFrame used for prediction.
        model: The trained binary machine learning model.

    Returns:
        pd.DataFrame: A DataFrame containing only the rows from the original df
                      where the model predicted a 'Win'.
  """
  # Predict on the feature set
  pred_result = model.predict(features)

  # Get the indices where the prediction is 'Win' (class 1)
  win_indices = features.iloc[pred_result == 1].index

  # Filter the original DataFrame using these indices
  df_filtered_win = df.loc[win_indices]

  return df_filtered_win

# Create a dictionary of binary models
binary_models = {
    'RandomForest': RF_model,
    'XGBoost': XVG_model,
    'Logistic Regression': LR_model,
    'Voting Classifier': VC_model,
    'Stacking Classifier': S_model
}

predicted_win_dfs = []

# Loop through each model, filter the data, and save to Excel
for model_name, model in binary_models.items():
    # Filter the X_test_P DataFrame (which contains the original columns for the test set)
    # based on the prediction from the corresponding binary model using the X_test features.
    df_trades_predicted_win = filter_trades_by_prediction(df, X_test, model)
    # print(df_trades_predicted_win.shape)
    predicted_win_dfs.append(df_trades_predicted_win)

    # Define the output filename
    output_filename = f'{model_name}_Predicted_Wins.xlsx'

    # Save the filtered DataFrame to an Excel file
    df_trades_predicted_win.to_excel(output_filename, index=False)

    print(f"Saved trades predicted as 'Win' by {model_name} to '{output_filename}'")

# Concatenate all dataframes into a single dataframe
combined_predicted_wins = pd.concat(predicted_win_dfs, ignore_index=True)
# Remove duplicate rows
combined_predicted_wins_unique = combined_predicted_wins.drop_duplicates(subset=['S/l'])

# Optionally, save the combined and unique dataframe to a new Excel file
combined_predicted_wins_unique.to_excel('All_Models_Predicted_Wins_Unique.xlsx', index=False)
print("\nCombined and unique predicted wins saved to 'All_Models_Predicted_Wins_Unique.xlsx'")
combined_predicted_wins_unique.shape

# prompt: predicited wins have all models predicted win in this order, [ 'RandomForest', 'XGBoost', 'Logistic Regression', 'Voting Classifier', 'Stacking Classifier',]. i want to combined losses of all other model except xgboost mean index 1 .. and then compare xgboost df losses with other combined losses

# Separate XGBoost predicted wins from the rest
xgboost_predicted_wins_df = predicted_win_dfs[1] # XGBoost is at index 1

# Combine predicted wins from all models EXCEPT XGBoost
other_models_predicted_wins_dfs = [df for i, df in enumerate(predicted_win_dfs) if i != 1]
combined_other_models_predicted_wins = pd.concat(other_models_predicted_wins_dfs, ignore_index=True)
combined_other_models_predicted_wins_unique = combined_other_models_predicted_wins.drop_duplicates(subset=['S/l'])

# Calculate losses for trades in each group
# A trade is a 'Loss' if 'Profit/Loss' is negative
xgboost_losses = xgboost_predicted_wins_df[xgboost_predicted_wins_df['Profit/Loss'] < 0]
other_combined_losses = combined_other_models_predicted_wins_unique[combined_other_models_predicted_wins_unique['Profit/Loss'] < 0]

# Compare the losses
print("\n--- Comparison of Losses ---")
print(f"Number of Losses in Trades Predicted as 'Win' by XGBoost: {len(xgboost_losses)}")
print(f"Total Loss Amount for XGBoost Predicted Wins: {xgboost_losses['Profit/Loss'].sum():.2f}")
print(f"Average Loss Amount for XGBoost Predicted Wins: {xgboost_losses['Profit/Loss'].mean():.2f}")

print(f"\nNumber of Losses in Trades Predicted as 'Win' by Other Combined Models: {len(other_combined_losses)}")
print(f"Total Loss Amount for Other Combined Models Predicted Wins: {other_combined_losses['Profit/Loss'].sum():.2f}")
print(f"Average Loss Amount for Other Combined Models Predicted Wins: {other_combined_losses['Profit/Loss'].mean():.2f}")

# prompt: show xgb model predicted win that actully win s/l difference betwwen other model combine predicted win and actually win uniuqe

# Filter trades predicted as 'Win' by XGBoost and see which ones were actually 'Win' or 'Loss'
xgboost_predicted_wins_actual_results = xgboost_predicted_wins_df[['S/l', 'Result', 'Profit/Loss']]


# Filter trades predicted as 'Win' by combined Other Models and see which ones were actually 'Win' or 'Loss'
other_combined_predicted_wins_actual_results = combined_other_models_predicted_wins_unique[['S/l', 'Result', 'Profit/Loss']]


# Find the unique S/l values where XGBoost predicted Win and the actual result was Win
xgboost_actual_wins_sl = xgboost_predicted_wins_df[xgboost_predicted_wins_df['Result'] == 'W']['S/l'].unique()
print(f"\nUnique S/l where XGBoost Predicted Win and Actually Won: {len(xgboost_actual_wins_sl)}")

# Find the unique S/l values where combined Other Models predicted Win and the actual result was Win
other_combined_actual_wins_sl = combined_other_models_predicted_wins_unique[combined_other_models_predicted_wins_unique['Result'] == 'W']['S/l'].unique()
print(f"Unique S/l where Other Combined Models Predicted Win (Unique S/l) and Actually Won: {len(other_combined_actual_wins_sl)}")

# Find the S/l values that are unique to XGBoost (predicted Win and actually Won)
unique_to_xgboost_actual_wins_sl = np.setdiff1d(xgboost_actual_wins_sl, other_combined_actual_wins_sl)
print(f"\nUnique S/l where ONLY XGBoost Predicted Win and Actually Won: {len(unique_to_xgboost_actual_wins_sl)}")
print(f"List of S/l unique to XGBoost actual wins: {unique_to_xgboost_actual_wins_sl.tolist()}")

# Find the S/l values that are unique to Other Combined Models (predicted Win and actually Won)
unique_to_other_combined_actual_wins_sl = np.setdiff1d(other_combined_actual_wins_sl, xgboost_actual_wins_sl)
print(f"\nUnique S/l where ONLY Other Combined Models Predicted Win (Unique S/l) and Actually Won: {len(unique_to_other_combined_actual_wins_sl)}")
print(f"List of S/l unique to Other Combined Models actual wins: {unique_to_other_combined_actual_wins_sl.tolist()}")

# Find the S/l values where both predicted Win and actually Won
common_actual_wins_sl = np.intersect1d(xgboost_actual_wins_sl, other_combined_actual_wins_sl)
print(f"\nUnique S/l where Both XGBoost and Other Combined Models Predicted Win and Actually Won: {len(common_actual_wins_sl)}")
print(f"List of S/l common actual wins: {common_actual_wins_sl.tolist()}")


# Filter trades predicted as 'Win' by XGBoost and the actual result was 'Loss'
xgboost_predicted_win_actual_loss_sl = xgboost_predicted_wins_df[xgboost_predicted_wins_df['Result'] == 'L']['S/l'].unique()
print(f"\nUnique S/l where XGBoost Predicted Win and Actually Lost: {len(xgboost_predicted_win_actual_loss_sl)}")

# Filter trades predicted as 'Win' by combined Other Models (unique S/l) and the actual result was 'Loss'
other_combined_predicted_win_actual_loss_sl = combined_other_models_predicted_wins_unique[combined_other_models_predicted_wins_unique['Result'] == 'L']['S/l'].unique()
print(f"Unique S/l where Other Combined Models Predicted Win (Unique S/l) and Actually Lost: {len(other_combined_predicted_win_actual_loss_sl)}")

# Find the unique S/l values in XGBoost predicted win and actually loss
unique_to_xgboost_predicted_win_actual_loss_sl = np.setdiff1d(xgboost_predicted_win_actual_loss_sl, other_combined_predicted_win_actual_loss_sl)
print(f"\nUnique S/l where ONLY XGBoost Predicted Win and Actually Lost: {len(unique_to_xgboost_predicted_win_actual_loss_sl)}")
print(f"List of S/l unique to XGBoost predicted win and actual loss: {unique_to_xgboost_predicted_win_actual_loss_sl.tolist()}")

# Find the S/l values that are unique to Other Combined Models (predicted Win and actually Loss)
unique_to_other_combined_predicted_win_actual_loss_sl = np.setdiff1d(other_combined_predicted_win_actual_loss_sl, xgboost_predicted_win_actual_loss_sl)
print(f"\nUnique S/l where ONLY Other Combined Models Predicted Win (Unique S/l) and Actually Lost: {len(unique_to_other_combined_predicted_win_actual_loss_sl)}")
print(f"List of S/l unique to Other Combined Models predicted win and actual loss: {unique_to_other_combined_predicted_win_actual_loss_sl.tolist()}")

# Find the S/l values where both predicted Win and actually Lost
common_predicted_win_actual_loss_sl = np.intersect1d(xgboost_predicted_win_actual_loss_sl, other_combined_predicted_win_actual_loss_sl)
print(f"\nUnique S/l where Both XGBoost and Other Combined Models Predicted Win and Actually Lost: {len(common_predicted_win_actual_loss_sl)}")
print(f"List of S/l common predicted win and actual loss: {common_predicted_win_actual_loss_sl.tolist()}")

# prompt: save other_combined_losses

# combined_other_models_predicted_wins_unique.to_excel('Other_Combined.xlsx', index=False)

RFS_LC = [predicted_win_dfs[0], predicted_win_dfs[2]]
RFS_LC_predicted_wins = pd.concat(RFS_LC, ignore_index=True)
RFS_LC_predicted_wins_unique = RFS_LC_predicted_wins.drop_duplicates(subset=['S/l'])
RFS_LC_predicted_wins_unique.to_excel('RFS_LC.xlsx', index=False)

"""##ML-Max R"""

df_max_r = pd.read_excel('SSL_T_MR.xlsx')

# prompt: count  of R value in df_max_r  that jave Result == 'W' greater than 5 and less then 9

# Filter the DataFrame where 'Result' is 'W'
df_wins = df_max_r[df_max_r['Result'] == 'W']

# Count occurrences where 'R' is greater than 5 and less than 9
r_count = df_wins[(df_wins['R'] > 8) & (df_wins['R'] < 100)].shape[0]

print(f"Count of 'R' values in df_max_r that have 'Result' == 'W' and are between 5 and 9: {r_count}")

df_max_r = prepare_data(df_max_r)
df_max_r = calculate_prior_streaks(df_max_r)

precomputed_stats_max_r = get_precomputed_stats(df_max_r)
train_data_max_r = get_train_data(df_max_r, precomputed_stats_max_r)
train_data_max_r = processing_train_data(df_max_r, train_data_max_r)
train_data_max_r = save_read_train_data(train_data_max_r, name = 'train_data_max_r')
# train_data_max_r = convert_train_data_type(train_data_max_r)

features_max_r, features_filtered_max_r, prediction_target_max_r = extract_ptarget_feature(train_data_max_r)
X_train_max_r, X_test_max_r, y_train_max_r, y_test_max_r, y_train_m_max_r, y_test_m_max_r, X_train_P_max_r, X_test_P_max_r= split_data(features_max_r, prediction_target_max_r)
show_split_data(X_train_max_r, X_test_max_r, y_train_max_r, y_test_max_r, y_train_m_max_r, y_test_m_max_r)

training_feature_cols = X_train_max_r.columns

RF_model_max_r,y_pred_RF_max_r, RF_accuracy_max_r, RF_class_report_max_r = RF(X_train_max_r, X_test_max_r, y_train_max_r, y_test_max_r, n_estimators=100)
show_model_result("Random Forest-Max R", RF_accuracy, RF_class_report)

XVG_model_max_r, y_pred_XVG_max_r, XVG_accuracy_max_r, XVG_class_report_max_r = XVG(X_train_max_r, X_test_max_r, y_train_max_r, y_test_max_r, eval_metric='logloss')
show_model_result("XGBoost-Max R", XVG_accuracy_max_r, XVG_class_report_max_r)

LR_model_max_r,y_pred_LR_max_r, LR_accuracy_max_r, LR_class_report_max_r = LR(X_train_max_r, X_test_max_r, y_train_max_r, y_test_max_r, solver='lbfgs', max_iter=1000)
show_model_result("Logistic Regression-Max R", LR_accuracy_max_r, LR_class_report_max_r)

VC_model_max_r,y_pred_VC_max_r, VC_accuracy_max_r, VC_class_report_max_r = VC(X_train_max_r, X_test_max_r, y_train_max_r, y_test_max_r, max_iter= 1000, eval_metric='logloss', voting='soft')
show_model_result("Voting Classifier-Max R", VC_accuracy_max_r, VC_class_report_max_r)

S_model_max_r,y_pred_S_max_r, S_accuracy_max_r, S_class_report_max_r = S(X_train_max_r, X_test_max_r, y_train_max_r, y_test_max_r, max_iter= 1000, eval_metric='logloss', cv=5, n_jobs=-1, ul_enc=False)
show_model_result("Stacking Classifier-Max R", S_accuracy_max_r, S_class_report_max_r)

# List of predictions from the trained models
all_predictions_max_r = [y_pred_RF_max_r, y_pred_XVG_max_r, y_pred_LR_max_r, y_pred_VC_max_r, y_pred_S_max_r]

# List of model names
model_names_max_r = ['RandomForest-Max R', 'XGBoost-Max R', 'Logistic Regression-Max R', 'Voting Classifier-Max R', 'Stacking Classifier-Max R']

# Plot the confusion matrices horizontally
plot_confusion_matrices_horizontal(y_test_max_r, all_predictions_max_r, model_names_max_r)

label_encoder_max_r, y_train_enc_max_r, y_test_enc_max_r, class_mapping_max_r, reverse_mapping_max_r = encode_train_test(y_train_m_max_r, y_test_m_max_r)

RF_model_m_max_r, y_pred_RF_m_max_r, RF_accuracy_m_max_r, RF_class_report_m_max_r = RF_M(X_train_max_r, y_train_enc_max_r, X_test_max_r, y_test_enc_max_r, n_estimators=100)
show_model_result("RandomForest Multi-Max R", RF_accuracy_m_max_r, RF_class_report_m_max_r)

XVG_model_m_max_r, y_pred_XVG_m_max_r, XVG_accuracy_m_max_r, XVG_class_report_m_max_r = XVG_M(X_train_max_r, y_train_enc_max_r, X_test_max_r, y_test_enc_max_r,num_class=prediction_target['Result_R'].nunique(), objective='multi:softmax', eval_metric = 'mlogloss', ul_enc = False)
show_model_result("XGBoost Multi-Max R", XVG_accuracy_m_max_r, XVG_class_report_m_max_r)

LR_model_m_max_r, y_pred_LR_m_max_r, LR_accuracy_m_max_r, LR_class_report_m_max_r = LR_M(X_train_max_r, y_train_enc_max_r, X_test_max_r, y_test_enc_max_r, solver='lbfgs', max_iter=10000)
show_model_result("Logistic Regression Multi-Max R", LR_accuracy_m_max_r, LR_class_report_m_max_r)

VC_model_m_max_r, y_pred_VC_m_max_r, VC_accuracy_m_max_r, VC_class_report_m_max_r = VC_M(X_train_max_r, y_train_enc_max_r, X_test_max_r, y_test_enc_max_r,num_class=prediction_target['Result_R'].nunique(), voting='soft', solver='lbfgs', max_iter=1000, objective='multi:softmax', eval_metric='mlogloss',ul_enc=False)
show_model_result("Voting Classifier Multi-Max R", VC_accuracy_m_max_r, VC_class_report_m_max_r)

S_model_m_max_r, y_pred_S_m_max_r, S_accuracy_m_max_r, S_class_report_m_max_r = S_M(X_train_max_r, y_train_enc_max_r, X_test_max_r, y_test_enc_max_r,num_class=prediction_target['Result_R'].nunique(), solver='lbfgs', max_iter=10000, objective='multi:softmax', eval_metric='mlogloss',cv =5, n_jobs=-1, ul_enc=False)
show_model_result("Stacking Classifier Multi-Max R", S_accuracy_m_max_r, S_class_report_m_max_r)

# List of predictions from the trained models
all_predictions_m_max_r = [y_pred_RF_m_max_r, y_pred_XVG_m_max_r, y_pred_LR_m_max_r, y_pred_VC_m_max_r, y_pred_S_m_max_r]
# List of model names
model_names_m_max_r = ['RandomForest Multi-Max R', 'XGBoost Multi-Max R', 'Logistic Regression Multi-Max R', 'Voting Classifier Multi-Max R', 'Stacking Classifier Multi-Max R']

# Plot the confusion matrices horizontally
plot_confusion_matrices_horizontal(y_test_enc_max_r, all_predictions_m_max_r, model_names_m_max_r ,ticks = sorted(np.unique(y_test_enc_max_r)))

time_input = "15:00"  #@param {type:"string"}
date_input = "6/27/2025"  #@param {type:"string"}
criteria_input = "ELC"  #@param {type:"string"}
CW = 0 #@param {type:"integer"}
CL = 1 #@param {type:"integer"}

model_names_max_r = ['RandomForest Max R', 'XGBoost Max R', 'Logistic Regression Max R', 'Voting Classifier Max R', 'Stacking Classifier Max R']
model_keys = ['RF', 'XVG', 'LR', 'VC', 'S']
show_predictions(time_input, date_input, criteria_input, CW, CL,training_feature_cols, model_names_max_r, model_keys, Max_R=True)
# List of predictions from the trained models
all_predictions_max_r = [y_pred_RF_max_r, y_pred_XVG_max_r, y_pred_LR_max_r, y_pred_VC_max_r, y_pred_S_max_r]

# Plot the confusion matrices horizontally
plot_confusion_matrices_horizontal(y_test_max_r, all_predictions_max_r, model_names_max_r)
result = Overall_R(df, time_input, date_input, criteria_input, with_overall=False)

overall_stats_df_max_r = overall_stats(X_test_P_max_r, X_test_max_r)
print("\nOverall Statistics for Trades Predicted as 'Win' by Each Model:")
overall_stats_df_max_r

# prompt: save all models

import joblib
import os

# Create a directory to save the models if it doesn't exist
model_dir = 'saved_models'
os.makedirs(model_dir, exist_ok=True)

# Define a dictionary of models to save
models_to_save = {
    'RF_model': RF_model,
    'XVG_model': XVG_model,
    'LR_model': LR_model,
    'VC_model': VC_model,
    'S_model': S_model,
    'RF_model_m': RF_model_m,
    'XVG_model_m': XVG_model_m,
    'LR_model_m': LR_model_m,
    'VC_model_m': VC_model_m,
    'S_model_m': S_model_m,
    # Include Max R models if they were successfully trained
    'RF_model_max_r': RF_model_max_r if 'RF_model_max_r' in globals() else None,
    'XVG_model_max_r': XVG_model_max_r if 'XVG_model_max_r' in globals() else None,
    'LR_model_max_r': LR_model_max_r if 'LR_model_max_r' in globals() else None,
    'VC_model_max_r': VC_model_max_r if 'VC_model_max_r' in globals() else None,
    'S_model_max_r': S_model_max_r if 'S_model_max_r' in globals() else None,
    'RF_model_m_max_r': RF_model_m_max_r if 'RF_model_m_max_r' in globals() else None,
    'XVG_model_m_max_r': XVG_model_m_max_r if 'XVG_model_m_max_r' in globals() else None,
    'LR_model_m_max_r': LR_model_m_max_r if 'LR_model_m_max_r' in globals() else None,
    'VC_model_m_max_r': VC_model_m_max_r if 'VC_model_m_max_r' in globals() else None,
    'S_model_m_max_r': S_model_m_max_r if 'S_model_m_max_r' in globals() else None,
    # Save the label encoder
    'label_encoder': label_encoder if 'label_encoder' in globals() else None,
    'label_encoder_max_r': label_encoder_max_r if 'label_encoder_max_r' in globals() else None,
}

# Save each model
for name, model in models_to_save.items():
    if model is not None: # Check if the model variable exists (especially for Max R models)
        filename = os.path.join(model_dir, f'{name}.pkl')
        try:
            joblib.dump(model, filename)
            print(f"Saved {name} to '{filename}'")
        except Exception as e:
            print(f"Error saving {name}: {e}")
    else:
        print(f"Model '{name}' not found, skipping save.")

# prompt: save model accuracies  separatly for max r as well and show import

# Combine accuracies into a single DataFrame for easy display
accuracy_data = {
    'Model': ['RandomForest', 'XGBoost', 'Logistic Regression', 'Voting Classifier', 'Stacking Classifier'],
    'Binary Accuracy': [RF_accuracy, XVG_accuracy, LR_accuracy, VC_accuracy, S_accuracy],
    'Multi-Class (R Bucket) Accuracy': [RF_accuracy_m, XVG_accuracy_m, LR_accuracy_m, VC_accuracy_m, S_accuracy_m]
}
accuracy_df = pd.DataFrame(accuracy_data)


# Save accuracies for the regular models
regular_model_accuracies = accuracy_df
regular_model_accuracies.to_excel("regular_model_accuracies.xlsx", index=False)
print("\nSaved regular model accuracies to 'regular_model_accuracies.xlsx'")

# Combine accuracies for Max R models
accuracy_data_max_r = {
    'Model': ['RandomForest Max R', 'XGBoost Max R', 'Logistic Regression Max R', 'Voting Classifier Max R', 'Stacking Classifier Max R'],
    'Binary Accuracy': [RF_accuracy_max_r, XVG_accuracy_max_r, LR_accuracy_max_r, VC_accuracy_max_r, S_accuracy_max_r],
    'Multi-Class (R Bucket) Accuracy': [RF_accuracy_m_max_r, XVG_accuracy_m_max_r, LR_accuracy_m_max_r, VC_accuracy_m_max_r, S_accuracy_m_max_r]
}
accuracy_df_max_r = pd.DataFrame(accuracy_data_max_r)
# Save accuracies for the Max R models
max_r_model_accuracies = accuracy_df_max_r
max_r_model_accuracies.to_excel("max_r_model_accuracies.xlsx", index=False)
print("\nSaved Max R model accuracies to 'max_r_model_accuracies.xlsx'")

"""#Read Require Data"""

# prompt: read both accuracies from save xlxs and show their tables

# Read the saved accuracy dataframes
try:
  precomputed_stats = joblib.load('precomputed_stats.pkl')
  training_feature_cols = joblib.load('training_feature_cols.pkl')
  regular_accuracies = pd.read_excel("regular_model_accuracies.xlsx")
  max_r_accuracies = pd.read_excel("max_r_model_accuracies.xlsx")

  print("\n--- Regular Model Accuracies ---")
  print(tabulate(regular_accuracies, headers='keys', tablefmt='psql'))

  print("\n--- Max R Model Accuracies ---")
  print(tabulate(max_r_accuracies, headers='keys', tablefmt='psql'))

except FileNotFoundError:
  print("Accuracy files not found. Please run the model training section first.")
except Exception as e:
  print(f"An error occurred while reading the accuracy files: {e}")

# prompt: read all 20 models separately  regular binary, regular multi, max r binary, max r multi, . so that i can pass binary model and multi models list to function easily

# Define a directory to save the models
model_dir = 'saved_models'

# Define lists to store the loaded models
regular_binary_models = []
regular_multi_models = []
max_r_binary_models = []
max_r_multi_models = []

# List of model base names (keys used in filenames)
model_keys = ['RF', 'XVG', 'LR', 'VC', 'S']

# Load regular binary models
print("Loading regular binary models...")
for key in model_keys:
    filename = os.path.join(model_dir, f'{key}_model.pkl')
    try:
        model = joblib.load(filename)
        regular_binary_models.append(model)
        print(f"Loaded {key}_model")
    except FileNotFoundError:
        print(f"Error: {key}_model.pkl not found. Please run the saving section first.")
    except Exception as e:
        print(f"Error loading {key}_model: {e}")

# Load regular multi-class models
print("\nLoading regular multi-class models...")
for key in model_keys:
    filename = os.path.join(model_dir, f'{key}_model_m.pkl')
    try:
        model = joblib.load(filename)
        regular_multi_models.append(model)
        print(f"Loaded {key}_model_m")
    except FileNotFoundError:
        print(f"Error: {key}_model_m.pkl not found. Please run the saving section first.")
    except Exception as e:
        print(f"Error loading {key}_model_m: {e}")

# Load max R binary models
print("\nLoading Max R binary models...")
for key in model_keys:
    filename = os.path.join(model_dir, f'{key}_model_max_r.pkl')
    try:
        model = joblib.load(filename)
        max_r_binary_models.append(model)
        print(f"Loaded {key}_model_max_r")
    except FileNotFoundError:
        print(f"Error: {key}_model_max_r.pkl not found. Please run the saving section first.")
    except Exception as e:
        print(f"Error loading {key}_model_max_r: {e}")

# Load max R multi-class models
print("\nLoading Max R multi-class models...")
for key in model_keys:
    filename = os.path.join(model_dir, f'{key}_model_m_max_r.pkl')
    try:
        model = joblib.load(filename)
        max_r_multi_models.append(model)
        print(f"Loaded {key}_model_m_max_r")
    except FileNotFoundError:
        print(f"Error: {key}_model_m_max_r.pkl not found. Please run the saving section first.")
    except Exception as e:
        print(f"Error loading {key}_model_m_max_r: {e}")

# Load Label Encoders
print("\nLoading Label Encoders...")
try:
    label_encoder = joblib.load(os.path.join(model_dir, 'label_encoder.pkl'))
    print("Loaded label_encoder")
except FileNotFoundError:
    print("Error: label_encoder.pkl not found.")
except Exception as e:
    print(f"Error loading label_encoder: {e}")

try:
    label_encoder_max_r = joblib.load(os.path.join(model_dir, 'label_encoder_max_r.pkl'))
    print("Loaded label_encoder_max_r")
except FileNotFoundError:
    print("Error: label_encoder_max_r.pkl not found.")
except Exception as e:
    print(f"Error loading label_encoder_max_r: {e}")


# Now you have the models loaded into these lists:
# regular_binary_models
# regular_multi_models
# max_r_binary_models
# max_r_multi_models

# You can pass these lists to other functions as needed.
# For example, you could create a function that takes a list of models and a dataframe
# and performs predictions or evaluation.

"""#Final Outputs"""

time_input = "08:00"  #@param {type:"string"}
date_input = "6/29/2025"  #@param {type:"string"}
criteria_input = "LG SPS"  #@param {type:"string"}
CW = 0 #@param {type:"integer"}
CL = 2 #@param {type:"integer"}

# show_predictions(time_input, date_input, criteria_input, CW, CL,training_feature_cols, model_names, model_keys, b_models=regular_binary_models, m_models=regular_multi_models, b_acc=regular_accuracies['Binary Accuracy'], m_acc=regular_accuracies['Multi-Class (R Bucket) Accuracy'], w_threshold=2)

# show_predictions(time_input, date_input, criteria_input, CW, CL,training_feature_cols, model_names_max_r, model_keys, b_models=max_r_binary_models, m_models=max_r_multi_models, b_acc=max_r_accuracies['Binary Accuracy'], m_acc=max_r_accuracies['Multi-Class (R Bucket) Accuracy'], Max_R=True, w_threshold=2)

show_predictions(time_input, date_input, criteria_input, CW, CL,training_feature_cols, model_names, model_keys, w_threshold=2)

show_predictions(time_input, date_input, criteria_input, CW, CL,training_feature_cols, model_names_max_r, model_keys, Max_R=True, w_threshold=2)

"""##Overall Stats Dataframe"""

overall_stats_df = overall_stats(X_test_P, X_test)
print("\nOverall Statistics for Trades Predicted as 'Win' by Each Model:")
overall_stats_df

overall_stats_df_max_r = overall_stats(X_test_P_max_r, X_test_max_r)
print("\nOverall Statistics for Trades Predicted as 'Win' by Each Model (Max_R):")
overall_stats_df_max_r

"""## Confustion Matrics"""

# prompt: plot all confusion matrics

# List of predictions from the trained models for the regular models
all_predictions_regular = [y_pred_RF, y_pred_XVG, y_pred_LR, y_pred_VC, y_pred_S]

# List of model names for the regular models
model_names_regular = ['RandomForest', 'XGBoost', 'Logistic Regression', 'Voting Classifier', 'Stacking Classifier']

# Plot the confusion matrices for regular binary models
plot_confusion_matrices_horizontal(y_test, all_predictions_regular, model_names_regular)

# List of predictions from the trained models for the regular multi-class models
all_predictions_multi_regular = [y_pred_RF_m, y_pred_XVG_m, y_pred_LR_m, y_pred_VC_m, y_pred_S_m]

# List of model names for the regular multi-class models
model_names_multi_regular = ['RandomForest Multi', 'XGBoost Multi', 'Logistic Regression Multi', 'Voting Classifier Multi', 'Stacking Classifier Multi']

# Plot the confusion matrices for regular multi-class models
plot_confusion_matrices_horizontal(y_test_enc, all_predictions_multi_regular, model_names_multi_regular, ticks=sorted(np.unique(y_test_enc)))

# List of predictions from the trained models for the Max R binary models
all_predictions_max_r = [y_pred_RF_max_r, y_pred_XVG_max_r, y_pred_LR_max_r, y_pred_VC_max_r, y_pred_S_max_r]

# List of model names for the Max R binary models
model_names_max_r = ['RandomForest-Max R', 'XGBoost-Max R', 'Logistic Regression-Max R', 'Voting Classifier-Max R', 'Stacking Classifier-Max R']

# Plot the confusion matrices for Max R binary models
plot_confusion_matrices_horizontal(y_test_max_r, all_predictions_max_r, model_names_max_r)

# List of predictions from the trained models for the Max R multi-class models
all_predictions_m_max_r = [y_pred_RF_m_max_r, y_pred_XVG_m_max_r, y_pred_LR_m_max_r, y_pred_VC_m_max_r, y_pred_S_m_max_r]

# List of model names for the Max R multi-class models
model_names_m_max_r = ['RandomForest Multi-Max R', 'XGBoost Multi-Max R', 'Logistic Regression Multi-Max R', 'Voting Classifier Multi-Max R', 'Stacking Classifier Multi-Max R']

# Plot the confusion matrices for Max R multi-class models
plot_confusion_matrices_horizontal(y_test_enc_max_r, all_predictions_m_max_r, model_names_m_max_r, ticks=sorted(np.unique(y_test_enc_max_r)))

"""##All Combination of Trade count, Win Rates, Profit/Loss"""

result = Overall_R(df, time_input, date_input, criteria_input, with_overall=False)
result_max_r = Overall_R(df, time_input, date_input, criteria_input, with_overall=False)

"""#Download All Saved Files"""

# prompt: download all files in save_models folder to my pc

from google.colab import files
import os

# Specify the directory you want to download from
directory_to_download = '/content/'

# Check if the directory exists
if os.path.exists(directory_to_download):
    # Create a zip file of the directory
    zip_filename = f"{directory_to_download}.zip"
    !zip -r {zip_filename} {directory_to_download}

    # Download the zip file
    files.download(zip_filename)
else:
    print(f"Directory '{directory_to_download}' does not exist.")